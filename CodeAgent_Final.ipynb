{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CodeAgent: LLM-based Agent Framework for Repository-level Code Generation\n",
        "\n",
        "This notebook is a **thin orchestrator** for the CodeAgent framework, a replication of the paper:\n",
        "[CodeAgent: Enhancing Code Generation with Tool-Integrated Agent Systems](https://arxiv.org/abs/2401.07339)\n",
        "\n",
        "## Overview\n",
        "\n",
        "CodeAgent leverages external tools to assist LLMs in repository-level code generation tasks. Unlike simple function-level generation, repository-level code generation requires understanding:\n",
        "- Documentation and README files\n",
        "- Code dependencies and imports\n",
        "- Runtime environment and testing infrastructure\n",
        "- Existing code patterns and conventions\n",
        "\n",
        "## Architecture\n",
        "\n",
        "The framework consists of five core programming tools:\n",
        "1. **FormatCheckTool**: Python code formatting using `black`\n",
        "2. **CodeSymbolNavigationTool**: AST-based code navigation using tree-sitter\n",
        "3. **CodeInterpreterTool**: Python code execution in isolated environments\n",
        "4. **DocSearchTool**: BM25-based documentation search\n",
        "5. **WebsiteSearchTool**: DuckDuckGo web search with summarization\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "codeagent/\n",
        "├── src/codeagent/\n",
        "│   ├── config/       # Configuration and settings\n",
        "│   ├── llm/          # LLM providers (HuggingFace, OpenAI, Gemini)\n",
        "│   ├── tools/        # Programming tools\n",
        "│   ├── benchmarks/   # Benchmark datasets\n",
        "│   ├── agents/       # Agent factory and strategies\n",
        "│   ├── evaluation/   # Evaluation pipeline\n",
        "│   └── utils/        # Utility functions\n",
        "├── tests/            # Test suite\n",
        "└── CodeAgent_Final.ipynb  # This orchestrator notebook\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 0: Setup & Configuration\n",
        "\n",
        "Initialize the environment, set random seeds for reproducibility, and configure project paths.\n",
        "\n",
        "This phase establishes a clean, consistent workspace and ensures experiments are deterministic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add the source directory to the Python path\n",
        "import sys\n",
        "sys.path.insert(0, \"./src\")\n",
        "\n",
        "# Core imports from the codeagent package\n",
        "from pathlib import Path\n",
        "from codeagent import (\n",
        "    CodeAgentConfig,\n",
        "    fix_random_seeds,\n",
        "    create_llm,\n",
        "    get_all_tools,\n",
        ")\n",
        "from codeagent.agents import create_agent_executor\n",
        "from codeagent.benchmarks import MiniTransformersBench, CodeAgentBench\n",
        "from codeagent.evaluation import (\n",
        "    run_evaluation_pipeline,\n",
        "    calculate_pass_rate,\n",
        "    generate_report,\n",
        "    print_summary,\n",
        ")\n",
        "\n",
        "# Configuration\n",
        "config = CodeAgentConfig(\n",
        "    project_repo_path=Path(\"./mini_transformers_repo\"),\n",
        "    random_seed=42,\n",
        ")\n",
        "\n",
        "# Fix random seeds for reproducibility\n",
        "fix_random_seeds(config.random_seed)\n",
        "\n",
        "print(f\"Project repository path: {config.project_repo_path}\")\n",
        "print(f\"Random seed: {config.random_seed}\")\n",
        "print(\"Setup complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 1: LLM Configuration\n",
        "\n",
        "Configure and load the LLM. The framework supports multiple providers:\n",
        "\n",
        "| Provider | Description | Model Examples |\n",
        "|----------|-------------|----------------|\n",
        "| **gemini** | Google's Gemini API | gemini-2.5-flash, gemini-pro |\n",
        "| **openai** | OpenAI's API | gpt-4, gpt-4-turbo, gpt-3.5-turbo |\n",
        "| **deepseek** | DeepSeek via OpenRouter | deepseek/deepseek-chat |\n",
        "| **huggingface** | Local models with quantization | codellama/CodeLlama-7b-hf |\n",
        "\n",
        "### API Key Configuration\n",
        "\n",
        "Set your API keys as environment variables:\n",
        "```bash\n",
        "export GOOGLE_API_KEY=\"your-gemini-key\"\n",
        "export OPENAI_API_KEY=\"your-openai-key\"\n",
        "export OPENROUTER_API_KEY=\"your-openrouter-key\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose your LLM provider: \"gemini\", \"openai\", \"deepseek\", or \"huggingface\"\n",
        "SELECTED_LLM = \"gemini\"\n",
        "\n",
        "# Optional: Specify a model ID (defaults are sensible for each provider)\n",
        "MODEL_ID = None  # e.g., \"gemini-2.5-flash\", \"gpt-4\", \"deepseek/deepseek-chat\"\n",
        "\n",
        "# Load the LLM\n",
        "llm, llm_ready = create_llm(\n",
        "    provider=SELECTED_LLM,\n",
        "    model_id=MODEL_ID,\n",
        ")\n",
        "\n",
        "if llm_ready:\n",
        "    print(f\"LLM '{SELECTED_LLM}' loaded successfully.\")\n",
        "else:\n",
        "    print(f\"Warning: LLM '{SELECTED_LLM}' failed to load. Check your API keys.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 2: Benchmark Loading\n",
        "\n",
        "Load the benchmark dataset. Two benchmarks are available:\n",
        "\n",
        "### MiniTransformers Benchmark (Recommended for Development)\n",
        "A smaller benchmark designed for iterative development and testing:\n",
        "- **22 source files** implementing a minimal transformer architecture\n",
        "- **15 tasks** covering additive, fix, and refactoring operations\n",
        "- **~98 words** average instruction length\n",
        "- Fast iteration cycles for debugging agent behavior\n",
        "\n",
        "### CodeAgentBench (Full Evaluation)\n",
        "The complete benchmark from the numpy-ml repository:\n",
        "- **57 tasks** (51 class generation, 6 function generation)\n",
        "- **~340 words** average instruction length\n",
        "- Complex files with up to 9,000 lines\n",
        "- Tests agent's ability to handle large codebases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose your benchmark: MiniTransformersBench (smaller) or CodeAgentBench (full)\n",
        "benchmark = MiniTransformersBench()\n",
        "\n",
        "# Load the codebase and tasks\n",
        "codebase_df = benchmark.load_codebase()\n",
        "tasks_df = benchmark.load_tasks()\n",
        "\n",
        "print(f\"Benchmark: {benchmark.__class__.__name__}\")\n",
        "print(f\"Codebase: {len(codebase_df)} files\")\n",
        "print(f\"Tasks: {len(tasks_df)} tasks\")\n",
        "print(f\"\\nTask IDs: {tasks_df['task_id'].tolist()}\")\n",
        "print(f\"\\nTask types:\")\n",
        "print(tasks_df.groupby('title')['task_id'].count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 3: Tools Setup\n",
        "\n",
        "Initialize the five programming tools that enable the agent to interact with the codebase:\n",
        "\n",
        "| Tool | Purpose | Key Technology |\n",
        "|------|---------|----------------|\n",
        "| **FormatCheck** | Validates Python code formatting | `black` formatter |\n",
        "| **CodeSymbolNavigation** | Searches/navigates code symbols | tree-sitter AST |\n",
        "| **CodeInterpreter** | Executes Python code safely | Isolated subprocess |\n",
        "| **DocSearch** | Searches documentation | BM25 ranking |\n",
        "| **WebSearch** | Searches the web | DuckDuckGo API |\n",
        "\n",
        "Each tool is designed to extend the LLM's capabilities beyond pure text generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create all tools configured for the project\n",
        "tools = get_all_tools(config.project_repo_path)\n",
        "\n",
        "print(f\"Initialized {len(tools)} tools:\")\n",
        "for tool in tools:\n",
        "    print(f\"\\n  {tool.name}:\")\n",
        "    print(f\"    {tool.description[:80]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 4: Agent Creation\n",
        "\n",
        "Create the agent executor with the chosen strategy. The agent orchestrates the LLM and tools.\n",
        "\n",
        "### Agent Strategies\n",
        "\n",
        "| Strategy | Description | Best For |\n",
        "|----------|-------------|----------|\n",
        "| **react** | ReAct with explicit Thought/Action/Observation | Debugging, understanding agent reasoning |\n",
        "| **tool_calling** | Native tool calling (OpenAI/Gemini) | Production use, faster execution |\n",
        "\n",
        "The ReAct strategy is recommended for development as it provides clear visibility into the agent's decision-making process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose agent strategy: \"react\" or \"tool_calling\"\n",
        "AGENT_STRATEGY = \"react\"\n",
        "\n",
        "# Create the agent executor\n",
        "if llm_ready:\n",
        "    agent_executor = create_agent_executor(\n",
        "        llm=llm,\n",
        "        tools=tools,\n",
        "        strategy=AGENT_STRATEGY,\n",
        "        project_repo_path=config.project_repo_path,\n",
        "        max_iterations=25,\n",
        "        verbose=True,\n",
        "    )\n",
        "    print(f\"Agent created with '{AGENT_STRATEGY}' strategy.\")\n",
        "    print(f\"Max iterations: 25\")\n",
        "else:\n",
        "    agent_executor = None\n",
        "    print(\"Skipping agent creation: LLM not ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 5: Evaluation Pipeline\n",
        "\n",
        "Run the evaluation pipeline on the benchmark tasks. For each task, the pipeline:\n",
        "\n",
        "1. **Repository Setup**: Reconstructs the codebase for the task\n",
        "2. **Agent Execution**: Runs the agent with the task prompt\n",
        "3. **Verification**: Runs pytest to verify the generated code\n",
        "4. **Result Collection**: Records pass/fail and logs\n",
        "\n",
        "### Configuration Options\n",
        "- `task_ids`: Run specific tasks only (default: all)\n",
        "- `start_from_task`: Resume from a specific task\n",
        "- `delay_between_tasks`: Rate limiting for API calls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Specify which tasks to run (None = all tasks)\n",
        "TASK_IDS_TO_RUN = None  # e.g., [\"miniformer-01\", \"miniformer-02\"]\n",
        "\n",
        "# Optional: Resume from a specific task\n",
        "START_FROM_TASK = None  # e.g., \"miniformer-05\"\n",
        "\n",
        "# Run the evaluation pipeline\n",
        "if agent_executor is not None:\n",
        "    results = run_evaluation_pipeline(\n",
        "        agent_executor=agent_executor,\n",
        "        codebase_df=codebase_df,\n",
        "        task_df=tasks_df,\n",
        "        project_repo_path=config.project_repo_path,\n",
        "        task_ids=TASK_IDS_TO_RUN,\n",
        "        start_from_task=START_FROM_TASK,\n",
        "        test_timeout=60,\n",
        "        delay_between_tasks=2.0,\n",
        "        print_results=True,\n",
        "        llm_name=SELECTED_LLM,\n",
        "    )\n",
        "else:\n",
        "    results = []\n",
        "    print(\"Skipping evaluation: Agent not configured.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 6: Results Analysis\n",
        "\n",
        "Analyze the evaluation results, generate reports, and examine failed tasks.\n",
        "\n",
        "Key metrics:\n",
        "- **Pass@1 Rate**: Percentage of tasks solved on first attempt\n",
        "- **Per-task breakdown**: Individual task outcomes\n",
        "- **Failure analysis**: Verification logs for debugging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if results:\n",
        "    # Generate summary report\n",
        "    report_df = generate_report(results)\n",
        "    display(report_df)\n",
        "    \n",
        "    # Calculate pass rate\n",
        "    pass_rate = calculate_pass_rate(results)\n",
        "    print(f\"\\nOverall Pass@1 Rate: {pass_rate:.2%}\")\n",
        "    print(f\"Passed: {sum(1 for r in results if r.success)}/{len(results)} tasks\")\n",
        "    \n",
        "    # Show failed tasks\n",
        "    failed = [r for r in results if not r.success]\n",
        "    if failed:\n",
        "        print(f\"\\n--- Failed Tasks ({len(failed)}) ---\")\n",
        "        for r in failed:\n",
        "            print(f\"  - {r.task_id}: {r.title}\")\n",
        "else:\n",
        "    print(\"No results to analyze.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 7: No-Agent Baseline (Optional)\n",
        "\n",
        "Run a baseline evaluation **without** the agent loop to measure the improvement from tool usage.\n",
        "\n",
        "The baseline:\n",
        "- Provides the LLM with all context (task description + existing code)\n",
        "- Asks for a single-shot code generation\n",
        "- No tool access, no iteration\n",
        "\n",
        "This demonstrates the value of the agentic approach with tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from codeagent.evaluation import run_no_agent_baseline, compare_results\n",
        "\n",
        "RUN_BASELINE = False  # Set to True to run baseline comparison\n",
        "\n",
        "if RUN_BASELINE and llm_ready:\n",
        "    print(\"Running no-agent baseline evaluation...\")\n",
        "    baseline_results = run_no_agent_baseline(\n",
        "        llm_instance=llm,\n",
        "        codebase_df=codebase_df,\n",
        "        task_df=tasks_df,\n",
        "        project_repo_path=config.project_repo_path,\n",
        "        task_ids=TASK_IDS_TO_RUN,\n",
        "        delay_between_tasks=2.0,\n",
        "    )\n",
        "    \n",
        "    # Compare agent vs baseline\n",
        "    if results:\n",
        "        comparison = compare_results(\n",
        "            agent_results=results,\n",
        "            baseline_results=baseline_results,\n",
        "            agent_name=f\"{SELECTED_LLM}_Agent\",\n",
        "            baseline_name=f\"{SELECTED_LLM}_NoAgent\",\n",
        "        )\n",
        "        display(comparison)\n",
        "else:\n",
        "    print(\"Baseline comparison skipped. Set RUN_BASELINE=True to enable.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 8: HumanEval Evaluation (Optional)\n",
        "\n",
        "Run the [HumanEval benchmark](https://github.com/openai/human-eval) for function-level code generation.\n",
        "\n",
        "HumanEval contains 164 Python programming problems testing:\n",
        "- Algorithm implementation\n",
        "- String manipulation\n",
        "- Data structure operations\n",
        "- Mathematical functions\n",
        "\n",
        "This provides a complementary evaluation to repository-level tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from codeagent.evaluation import run_full_humaneval_pipeline\n",
        "\n",
        "RUN_HUMANEVAL = False  # Set to True to run HumanEval\n",
        "HUMANEVAL_SAMPLES = 15  # Number of problems to run (None = all 164)\n",
        "\n",
        "if RUN_HUMANEVAL and agent_executor is not None:\n",
        "    print(f\"Running HumanEval evaluation ({HUMANEVAL_SAMPLES or 164} problems)...\")\n",
        "    humaneval_results = run_full_humaneval_pipeline(\n",
        "        agent_executor=agent_executor,\n",
        "        output_dir=Path(\"./evaluation_results/HumanEval\"),\n",
        "        num_samples=HUMANEVAL_SAMPLES,\n",
        "        llm_name=SELECTED_LLM,\n",
        "        delay_between_tasks=5.0,\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nHumanEval Pass@1: {humaneval_results['results'].get('pass@1', 0):.2%}\")\n",
        "else:\n",
        "    print(\"HumanEval evaluation skipped. Set RUN_HUMANEVAL=True to enable.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook demonstrated the complete CodeAgent workflow:\n",
        "\n",
        "| Phase | Component | Purpose |\n",
        "|-------|-----------|----------|\n",
        "| 0 | Setup | Configuration, seeds, paths |\n",
        "| 1 | LLM | Load language model |\n",
        "| 2 | Benchmark | Load evaluation tasks |\n",
        "| 3 | Tools | Initialize programming tools |\n",
        "| 4 | Agent | Create agent executor |\n",
        "| 5 | Evaluation | Run evaluation pipeline |\n",
        "| 6 | Analysis | Examine results and metrics |\n",
        "| 7 | Baseline | Compare with no-tool approach |\n",
        "| 8 | HumanEval | Function-level evaluation |\n",
        "\n",
        "### Key Findings from the Paper\n",
        "\n",
        "1. **Tools Matter**: The agent with tools significantly outperforms the base LLM\n",
        "2. **FormatCheck Impact**: Code formatting validation catches many errors early\n",
        "3. **CodeInterpreter Value**: Runtime testing enables iterative debugging\n",
        "4. **Context is Key**: Repository-level context enables accurate generation\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Experiment with different LLM providers\n",
        "- Run the full CodeAgentBench for comprehensive evaluation\n",
        "- Analyze tool usage patterns to understand agent behavior\n",
        "- Compare performance across model sizes"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
