{"path": "main.py", "content": ["# Main entry point for agent tasks. Initially empty."]}
{"path": "README.md", "content": ["# Miniformer\n", "A minimal, educational library for building transformer blocks, designed for agent-based code generation tasks.\n", "This library provides core components for building and experimenting with transformer architectures."]}
{"path": "requirements.txt", "content": ["numpy\n", "torch\n", "scipy\n", "pydantic"]}
{"path": "miniformer/__init__.py", "content": ["from .models.block import TransformerBlock"]}
{"path": "miniformer/config.py", "content": ["from pydantic import BaseModel, Field\n", "from typing import Literal\n", "\n", "class TransformerConfig(BaseModel):\n", "    \"\"\"Configuration for a Miniformer model.\"\"\"\n", "    vocab_size: int = Field(default=1000, ge=1)\n", "    n_layer: int = Field(default=4, ge=1)\n", "    n_head: int = Field(default=4, ge=1)\n", "    n_embd: int = Field(default=128, ge=1)\n", "    block_size: int = Field(default=256, ge=1)\n", "    attn_pdrop: float = Field(default=0.1, ge=0.0, le=1.0)\n", "    resid_pdrop: float = Field(default=0.1, ge=0.0, le=1.0)\n", "    activation_function: Literal['relu', 'gelu'] = 'gelu'\n", "\n", "    class Config:\n", "        validate_assignment = True\n", "\n", "    def __post_init__(self):\n", "        if self.n_embd % self.n_head != 0:\n", "            raise ValueError(\"n_embd must be divisible by n_head\")"]}
{"path": "miniformer/layers/__init__.py", "content": ["from .attention import MultiHeadSelfAttention"]}
{"path": "miniformer/layers/attention.py", "content": ["import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import math\n", "\n", "class CausalSelfAttention(nn.Module):\n", "    \"\"\"A vanilla multi-head masked self-attention layer with a projection at the end.\"\"\"\n", "    def __init__(self, config):\n", "        super().__init__()\n", "        assert config.n_embd % config.n_head == 0\n", "        # Key, query, value projections for all heads, but in a batch\n", "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n", "        # Output projection\n", "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n", "        # Regularization\n", "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n", "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n", "        # Causal mask\n", "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n", "                                      .view(1, 1, config.block_size, config.block_size))\n", "        self.n_head = config.n_head\n", "        self.n_embd = config.n_embd\n", "\n", "    def forward(self, x):\n", "        B, T, C = x.size() # Batch size, sequence length, embedding dimensionality\n", "        # Calculate query, key, values for all heads in batch and move head forward to be the batch dim\n", "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n", "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n", "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n", "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n", "\n", "        # Causal self-attention\n", "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n", "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n", "        att = F.softmax(att, dim=-1)\n", "        att = self.attn_dropout(att)\n", "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n", "        y = y.transpose(1, 2).contiguous().view(B, T, C) # Re-assemble all head outputs\n", "        # Output projection\n", "        y = self.resid_dropout(self.c_proj(y))\n", "        return y\n", "\n", "class MultiHeadSelfAttention(CausalSelfAttention):\n", "    \"\"\"Alias for CausalSelfAttention for clearer naming conventions.\"\"\"\n", "    pass"]}
{"path": "miniformer/layers/feedforward.py", "content": ["import torch.nn as nn\n", "from miniformer.activations import get_activation\n", "\n", "class FeedForward(nn.Module):\n", "    \"\"\"A position-wise feed-forward network.\"\"\"\n", "    def __init__(self, config):\n", "        super().__init__()\n", "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n", "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n", "        self.dropout = nn.Dropout(config.resid_pdrop)\n", "        self.activation = get_activation(config.activation_function)\n", "\n", "    def forward(self, x):\n", "        return self.dropout(self.c_proj(self.activation(self.c_fc(x))))"]}
{"path": "miniformer/activations.py", "content": ["import torch\n", "import torch.nn.functional as F\n", "from scipy.special import erf\n", "import numpy as np\n", "\n", "def gelu_exact(x):\n", "    \"\"\"Gaussian Error Linear Unit (GELU) activation function using SciPy's erf for NumPy arrays.\"\"\"\n", "    return 0.5 * x * (1.0 + erf(x / np.sqrt(2.0)))\n", "\n", "def get_activation(name: str):\n", "    \"\"\"Returns the activation function corresponding to the name.\"\"\"\n", "    if name == 'relu':\n", "        return F.relu\n", "    elif name == 'gelu':\n", "        return F.gelu # PyTorch's optimized GELU for tensors\n", "    else:\n", "        raise ValueError(f\"Unknown activation function: {name}\")"]}
{"path": "miniformer/models/__init__.py", "content": ["from .block import TransformerBlock"]}
{"path": "miniformer/models/block.py", "content": ["import torch.nn as nn\n", "from miniformer.layers.attention import MultiHeadSelfAttention\n", "from miniformer.layers.feedforward import FeedForward\n", "\n", "class TransformerBlock(nn.Module):\n", "    \"\"\"\n", "    A single block of a transformer model. It consists of a multi-head\n", "    self-attention layer followed by a feed-forward network. Layer\n", "    normalization and residual connections are applied.\n", "    \"\"\"\n", "    def __init__(self, config):\n", "        super().__init__()\n", "        self.ln_1 = nn.LayerNorm(config.n_embd)\n", "        self.attn = MultiHeadSelfAttention(config)\n", "        self.ln_2 = nn.LayerNorm(config.n_embd)\n", "        self.mlp = FeedForward(config)\n", "\n", "    def forward(self, x):\n", "        x = x + self.attn(self.ln_1(x))\n", "        x = x + self.mlp(self.ln_2(x))\n", "        return x"]}
{"path": "miniformer/utils/__init__.py", "content": ["from .testing import assert_allclose"]}
{"path": "miniformer/utils/testing.py", "content": ["import numpy as np\n", "import time\n", "\n", "def assert_allclose(actual, desired, rtol=1e-6, atol=1e-6, label=\"\"):\n", "    \"\"\"A wrapper around np.testing.assert_allclose with a label for better test reporting.\"\"\"\n", "    try:\n", "        np.testing.assert_allclose(actual, desired, rtol=rtol, atol=atol)\n", "        if label: print(f\"Assertion PASSED for: {label}\")\n", "    except AssertionError as e:\n", "        print(f\"Assertion FAILED for: {label}\" if label else \"Assertion FAILED.\")\n", "        raise e"]}
{"path": "miniformer/utils/tensor_ops.py", "content": ["import numpy as np\n", "import math\n", "import torch\n", "\n", "def to_numpy(tensor):\n", "    \"\"\"Converts a PyTorch tensor to a NumPy array on the CPU.\"\"\"\n", "    if tensor is None: return None\n", "    return tensor.detach().cpu().numpy()\n", "\n", "def kaiming_uniform_numpy(shape, a=0, mode='fan_in', nonlinearity='leaky_relu'):\n", "    \"\"\"A simplified numpy-based Kaiming uniform initializer.\"\"\"\n", "    fan = np.prod(shape[1:]) if mode == 'fan_in' else shape[0]\n", "    gain = math.sqrt(2.0 / (1 + a**2)) if nonlinearity == 'leaky_relu' else 1.0\n", "    std = gain / math.sqrt(fan)\n", "    bound = math.sqrt(3.0) * std\n", "    return np.random.uniform(-bound, bound, size=shape)"]}
{"path": "tests/__init__.py", "content": [""]}
{"path": "tests/test_integration.py", "content": ["# ==============================================================================\n", "# Comprehensive Integration Test Suite for the Miniformer Library\n", "# ==============================================================================\n", "#\n", "# This file serves as the primary validation mechanism for the entire miniformer\n", "# codebase. Unlike unit tests that might focus on a single function, these\n", "# integration tests ensure that all the different components (config, layers,\n", "# models, utils) work together as expected.\n", "#\n", "# This file is intentionally verbose and detailed to:\n", "# 1. Provide a clear example of how to use the library's components.\n", "# 2. Serve as a challenging, realistic file for a code generation agent to modify.\n", "# 3. Create a statistical outlier in file size, mimicking real-world codebases.\n", "#\n", "# It imports from nearly every other module in the project.\n", "\n", "import torch\n", "import numpy as np\n", "import time\n", "import pytest # Using pytest for better test structure and fixtures\n", "\n", "# Import all major components from the library\n", "from miniformer.config import TransformerConfig\n", "from miniformer.models.block import TransformerBlock\n", "from miniformer.layers.attention import MultiHeadSelfAttention\n", "from miniformer.layers.feedforward import FeedForward\n", "from miniformer.activations import get_activation\n", "from miniformer.utils.testing import assert_allclose\n", "from miniformer.utils.tensor_ops import to_numpy\n", "\n", "# --- Test Fixtures ---\n", "\n", "@pytest.fixture(scope=\"module\")\n", "def default_config():\n", "    \"\"\"Provides a default, consistent TransformerConfig for all tests in this module.\"\"\"\n", "    return TransformerConfig(\n", "        vocab_size=100,\n", "        n_layer=2,\n", "        n_head=4,\n", "        n_embd=64,\n", "        block_size=128,\n", "        attn_pdrop=0.0,  # Disable dropout for deterministic tests\n", "        resid_pdrop=0.0\n", "    )\n", "\n", "@pytest.fixture\n", "def transformer_block(default_config):\n", "    \"\"\"Provides an initialized TransformerBlock instance for testing.\"\"\"\n", "    model = TransformerBlock(default_config)\n", "    model.eval() # Set to evaluation mode to disable dropout behavior\n", "    return model\n", "\n", "\n", "# --- Core Functionality Tests ---\n", "\n", "def test_block_forward_pass_shape(transformer_block, default_config):\n", "    \"\"\"\n", "    PURPOSE: To verify that a forward pass through a TransformerBlock preserves\n", "    the shape of the input tensor. This is the most fundamental sanity check.\n", "    If input shape is (B, T, C), output shape must also be (B, T, C).\n", "    \"\"\"\n", "    print(\"\\n--- Running Test: Block Forward Pass Shape ---\")\n", "    batch_size = 4\n", "    seq_len = 32\n", "    input_tensor = torch.rand(batch_size, seq_len, default_config.n_embd)\n", "\n", "    with torch.no_grad():\n", "        output_tensor = transformer_block(input_tensor)\n", "\n", "    assert input_tensor.shape == output_tensor.shape, \\\n", "        f\"Shape mismatch: In {input_tensor.shape} vs Out {output_tensor.shape}\"\n", "    print(\"Shape preservation test PASSED.\")\n", "\n", "\n", "def test_attention_causality(default_config):\n", "    \"\"\"\n", "    PURPOSE: To rigorously test the causal nature of the self-attention mechanism.\n", "    A token at position `i` should NEVER be influenced by tokens at positions `j > i`.\n", "\n", "    METHOD:\n", "    1. Create an input tensor `input1`.\n", "    2. Create a second tensor `input2` which is identical to `input1` initially.\n", "    3. Modify `input2` at a future position (e.g., add noise at `t+1`).\n", "    4. Pass both tensors through the attention layer.\n", "    5. The output for position `t` should be IDENTICAL for both `output1` and `output2`.\n", "    \"\"\"\n", "    print(\"\\n--- Running Test: Attention Causality ---\")\n", "    attention_layer = MultiHeadSelfAttention(default_config).eval()\n", "    seq_len = 16\n", "    pos_to_check = 8 # The position we will observe\n", "\n", "    # Create two input tensors.\n", "    input1 = torch.randn(1, seq_len, default_config.n_embd)\n", "    input2 = input1.clone()\n", "    input2[:, pos_to_check + 1, :] += 10.0 # Add large noise to a future token\n", "\n", "    with torch.no_grad():\n", "        output1 = attention_layer(input1)\n", "        output2 = attention_layer(input2)\n", "\n", "    # The output up to and including `pos_to_check` should be bit-for-bit identical.\n", "    out1_numpy = to_numpy(output1[:, :pos_to_check + 1, :])\n", "    out2_numpy = to_numpy(output2[:, :pos_to_check + 1, :])\n", "\n", "    assert_allclose(out1_numpy, out2_numpy, label=\"Causality Check\")\n", "    print(\"Causality test PASSED.\")\n", "\n", "def test_batch_independence(transformer_block):\n", "    \"\"\"\n", "    PURPOSE: To ensure that computations for different items in a batch are\n", "    completely independent of one another.\n", "\n", "    METHOD:\n", "    1. Process a full batch of size N > 1.\n", "    2. Process the first item of that batch alone (batch size 1).\n", "    3. The output from the single-item pass must be identical to the first slice\n", "       of the output from the full-batch pass.\n", "    \"\"\"\n", "    print(\"\\n--- Running Test: Batch Independence ---\")\n", "    # Input with batch size > 1\n", "    full_batch_input = torch.rand(4, 16, transformer_block.attn.n_embd)\n", "    # Input with only the first element of the batch\n", "    single_item_input = full_batch_input[0:1, :, :].clone()\n", "\n", "    with torch.no_grad():\n", "        full_batch_output = transformer_block(full_batch_input)\n", "        single_item_output = transformer_block(single_item_input)\n", "\n", "    # Compare the first item from the full batch output to the single item output\n", "    out_full_numpy = to_numpy(full_batch_output[0:1, :, :])\n", "    out_single_numpy = to_numpy(single_item_output)\n", "\n", "    assert_allclose(out_full_numpy, out_single_numpy, label=\"Batch Independence\")\n", "    print(\"Batch independence test PASSED.\")\n", "\n", "# --- A placeholder for future tests ---\n", "def test_model_training_step():\n", "    \"\"\"\n", "    PURPOSE: This is a placeholder test. A real implementation would check if the\n", "    model parameters are updated after a backward pass and optimizer step.\n", "    For this benchmark, we just confirm that it runs without error.\n", "    \"\"\"\n", "    print(\"\\n--- Running Test: Model Training Step (Placeholder) ---\")\n", "    assert True\n", "    print(\"Training step placeholder test PASSED.\")\n", "\n", "# --- Main execution block to run tests if the file is executed directly ---\n", "\n", "def run_all_tests():\n", "    \"\"\"Main function to run all defined tests sequentially.\"\"\"\n", "    # This function is more for direct execution than for pytest\n", "    config = default_config()\n", "    block = transformer_block(config)\n", "\n", "    test_block_forward_pass_shape(block, config)\n", "    test_attention_causality(config)\n", "    test_batch_independence(block)\n", "    test_model_training_step()\n", "\n", "\n", "if __name__ == \"__main__\":\n", "    print(\"=============================================\")\n", "    print(\"    RUNNING MINIFORMER INTEGRATION SUITE     \")\n", "    print(\"=============================================\")\n", "    start_time = time.time()\n", "\n", "    # Manually run tests if not using pytest\n", "    run_all_tests()\n", "\n", "    end_time = time.time()\n", "    print(\"\\n=============================================\")\n", "    print(f\"   SUITE FINISHED in {end_time - start_time:.2f}s\")\n", "    print(\"=============================================\")"]}
{"path": "tests/test_config.py", "content": ["import pytest\n", "from miniformer.config import TransformerConfig\n", "\n", "def test_config_instantiation():\n", "    # Tests that a config can be created with custom values (Task #11)\n", "    config = TransformerConfig(n_layer=2, n_head=2, n_embd=32)\n", "    assert config.n_layer == 2\n", "    assert config.n_head == 2\n", "    assert config.n_embd == 32\n", "\n", "def test_config_to_dict_method():\n", "    # This test is expected to fail until Task #3 is completed.\n", "    config = TransformerConfig()\n", "    try:\n", "        d = config.to_dict()\n", "        assert isinstance(d, dict)\n", "        assert d['n_embd'] == 128\n", "    except AttributeError:\n", "        pytest.fail(\"The 'to_dict' method does not exist on TransformerConfig.\")\n", "\n", "def test_config_bias_field():\n", "    # This test is expected to fail until Task #1 is completed.\n", "    try:\n", "        config = TransformerConfig(use_bias=True)\n", "        assert config.use_bias is True\n", "    except TypeError:\n", "        pytest.fail(\"The 'use_bias' field does not exist on TransformerConfig.\")"]}
{"path": "tests/test_activations.py", "content": ["import pytest\n", "import torch\n", "from miniformer.activations import get_activation\n", "\n", "def test_swish_activation():\n", "    # This test is expected to fail until Task #4 is completed.\n", "    try:\n", "        swish = get_activation('swish')\n", "        x = torch.tensor([1.0, 2.0, -1.0])\n", "        expected = x * torch.sigmoid(x)\n", "        assert torch.allclose(swish(x), expected)\n", "    except ValueError:\n", "        pytest.fail(\"Activation 'swish' is not registered in get_activation.\")"]}
{"path": "tests/test_layers.py", "content": ["import pytest\n", "import torch\n", "from miniformer.config import TransformerConfig\n", "\n", "def test_flash_attention_placeholder():\n", "    # This test is expected to fail until Task #12 is completed.\n", "    from miniformer.layers.attention import CausalSelfAttention\n", "\n", "    # This config would enable flash attention if the field existed\n", "    try:\n", "        config = TransformerConfig(use_flash=True, n_embd=32, n_head=4)\n", "        attention = CausalSelfAttention(config)\n", "        input_tensor = torch.rand(1, 16, 32)\n", "        with pytest.raises(NotImplementedError, match=\"Flash Attention not yet implemented\"):\n", "            attention(input_tensor)\n", "    except (TypeError, AttributeError):\n", "        pytest.fail(\"Task #12 is not complete. Either 'use_flash' field is missing or the check in CausalSelfAttention is not implemented.\")"]}
{"path": "tests/test_models.py", "content": ["import pytest\n", "from miniformer.config import TransformerConfig\n", "from miniformer.models.block import TransformerBlock\n", "\n", "def test_block_summary_method():\n", "    # This test is expected to fail until Task #9 is completed.\n", "    config = TransformerConfig()\n", "    block = TransformerBlock(config)\n", "    try:\n", "        # Check if the method exists and returns a string containing layer names\n", "        summary_str = block.summary()\n", "        assert isinstance(summary_str, str)\n", "        assert \"MultiHeadSelfAttention\" in summary_str\n", "        assert \"FeedForward\" in summary_str\n", "    except (AttributeError, TypeError):\n", "        pytest.fail(\"The 'summary' method does not exist or does not return a string.\")"]}
{"path": "tests/test_utils.py", "content": ["import torch\n", "import numpy as np\n", "from miniformer.utils.tensor_ops import to_numpy\n", "\n", "def test_to_numpy_conversion():\n", "    # Tests the functionality for Task #2\n", "    tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n", "    arr = to_numpy(tensor)\n", "    assert isinstance(arr, np.ndarray)\n", "    assert arr.shape == (2, 2)\n", "\n", "def test_xavier_initializer():\n", "    # This test is expected to fail until Task #14 (the replacement task) is completed.\n", "    try:\n", "        from miniformer.utils.tensor_ops import xavier_uniform_numpy\n", "        shape = (100, 100)\n", "        result = xavier_uniform_numpy(shape)\n", "        assert result.shape == shape\n", "        # Check if values are within a reasonable bound for uniform dist\n", "        assert np.abs(result).mean() < 0.5\n", "    except ImportError:\n", "        pytest.fail(\"Function 'xavier_uniform_numpy' not found in tensor_ops.py.\")"]}
{"path": "tests/test_created_files.py", "content": ["import pytest\n", "import torch\n", "from miniformer.config import TransformerConfig\n", "\n", "# This file contains tests for components that are created by agent tasks.\n", "# These tests are EXPECTED TO FAIL until the agent completes the tasks.\n", "\n", "def test_positional_embedding_layer():\n", "    # This test is for Task #5.\n", "    try:\n", "        from miniformer.layers.embedding import PositionalEmbedding\n", "        config = TransformerConfig(block_size=64, n_embd=32)\n", "        layer = PositionalEmbedding(config)\n", "        input_tensor = torch.rand(2, 16, 32) # B, T, C\n", "        pos_emb = layer(input_tensor)\n", "        assert pos_emb.shape == (16, 32) # T, C\n", "    except ImportError:\n", "        pytest.fail(\"File 'miniformer/layers/embedding.py' or class 'PositionalEmbedding' not found.\")\n", "\n", "def test_full_model_instantiation():\n", "    # This test is for Task #10.\n", "    try:\n", "        from miniformer.models.model import Miniformer\n", "        config = TransformerConfig()\n", "        model = Miniformer(config)\n", "        assert model is not None\n", "    except ImportError:\n", "        pytest.fail(\"File 'miniformer/models/model.py' or class 'Miniformer' not found.\")\n", "\n", "def test_language_model_head():\n", "    # This test is for Task #15.\n", "    try:\n", "        from miniformer.models.head import LanguageModelHead\n", "        config = TransformerConfig(n_embd=32, vocab_size=100)\n", "        head = LanguageModelHead(config)\n", "        input_tensor = torch.rand(2, 16, 32) # B, T, C\n", "        logits = head(input_tensor)\n", "        assert logits.shape == (2, 16, 100) # B, T, vocab_size\n", "    except ImportError:\n", "        pytest.fail(\"File 'miniformer/models/head.py' or class 'LanguageModelHead' not found.\")"]}
