{"title": "TransformerConfig", "class_annotation": "miniformer.config.TransformerConfig", "comment": "\"Add Bias Configuration to TransformerConfig\"\n\nclass miniformer.config.TransformerConfig\n\nModify the configuration class to support toggling bias terms in linear layers.\n\n-[ Notes ]-\nIn some transformer architectures, bias terms in linear projections are considered redundant, especially when followed by a normalization layer like LayerNorm. Providing a configurable flag for this is a common design pattern for architectural experimentation. Your change is required to make the test `test_config_bias_field` in `tests/test_config.py` pass.\n\nParameters to Add:\n  * **use_bias** (*bool*) -- If True, linear layers will include a bias term. This should default to `False` to follow modern best practices.", "class_name": "miniformer.config.TransformerConfig", "class_link": "miniformer/config.py", "test_file_path": "tests/test_config.py", "task_id": "miniformer-01"}
{"title": "to_numpy", "class_annotation": "miniformer.utils.tensor_ops.to_numpy", "comment": "\"Verify Tensor Conversion Utility\"\n\nfunction miniformer.utils.tensor_ops.to_numpy(tensor)\n\nVerify that the `to_numpy` utility function is correctly implemented.\n\n-[ Description ]-\nThis function serves as a standard bridge between PyTorch tensors and NumPy arrays, a common operation in ML workflows for debugging, analysis, or interfacing with other libraries. It should handle the device transfer (`.cpu()`) and gradient detachment (`.detach()`) before conversion. The test `test_to_numpy_conversion` in `tests/test_utils.py` validates this behavior. No changes are needed if the implementation is correct.", "class_name": "N/A", "class_link": "miniformer/utils/tensor_ops.py", "test_file_path": "tests/test_utils.py", "task_id": "miniformer-02"}
{"title": "TransformerConfig", "class_annotation": "miniformer.config.TransformerConfig", "comment": "\"Implement Configuration Export Method\"\n\nclass miniformer.config.TransformerConfig\n\nAdd a method to the `TransformerConfig` class for exporting its settings.\n\nMethod to Add:\n  to_dict(self)\n    Exports the configuration instance to a Python dictionary. This is crucial for serialization (e.g., saving to JSON), logging experiment parameters, or re-instantiating models from a saved state. The implementation should leverage the built-in `.dict()` method from the Pydantic `BaseModel`. This is required to pass the `test_config_to_dict_method`.", "class_name": "miniformer.config.TransformerConfig", "class_link": "miniformer/config.py", "test_file_path": "tests/test_config.py", "task_id": "miniformer-03"}
{"title": "get_activation", "class_annotation": "miniformer.activations.get_activation", "comment": "\"Add Swish Activation Support\"\n\nfunction miniformer.activations.get_activation(name)\n\nExtend the activation function factory to include support for 'swish'.\n\n-[ Notes ]-\nThe Swish activation function, defined as `f(x) = x * sigmoid(x)`, is a smooth, non-monotonic function that often matches or exceeds the performance of ReLU on deeper models.\n\nImplementation Steps:\n1.  Implement a new Python function `swish(x)` that computes the activation. It should use `torch.sigmoid`.\n2.  Modify the `get_activation` function to return a reference to your `swish` function when the input `name` is 'swish'.\nThis change is required to pass `test_swish_activation` in `tests/test_activations.py`.", "class_name": "N/A", "class_link": "miniformer/activations.py", "test_file_path": "tests/test_activations.py", "task_id": "miniformer-04"}
{"title": "PositionalEmbedding", "class_annotation": "miniformer.layers.embedding.PositionalEmbedding", "comment": "\"Create PositionalEmbedding Layer\"\n\nclass miniformer.layers.embedding.PositionalEmbedding(config)\n\nBases: `torch.nn.Module`\n\nCreate a new file `miniformer/layers/embedding.py`. In it, define a `PositionalEmbedding` class.\n\n-[ Description ]-\nThis layer learns a unique vector for each position in the input sequence, up to a maximum length defined by `config.block_size`. These embeddings are added to token embeddings to provide the model with information about token order, which is essential for sequence processing tasks since self-attention is permutation-invariant. This implementation is required to pass `test_positional_embedding_layer`.\n\n__init__(self, config):\n  The constructor must initialize a `torch.nn.Embedding` layer.\n  Parameters:\n    * **num_embeddings** (*int*): The maximum number of positions, from `config.block_size`.\n    * **embedding_dim** (*int*): The dimensionality of the embedding vectors, from `config.n_embd`.\n\nforward(self, x):\n  The forward pass must generate the positional embeddings for the sequence length `T` of the input tensor `x`.\n  Parameters:\n    * **x** (*torch.Tensor* of shape *(B, T, C)*): The input tensor from the previous layer. Only the sequence length `T` is used.\n  Returns:\n    * **pos_emb** (*torch.Tensor* of shape *(T, C)*): The learned positional embeddings for positions 0 to T-1.", "class_name": "miniformer.layers.embedding.PositionalEmbedding", "class_link": "miniformer/layers/embedding.py", "test_file_path": "tests/test_created_files.py", "task_id": "miniformer-05"}
{"title": "MultiHeadSelfAttention", "class_annotation": "miniformer.layers.attention.MultiHeadSelfAttention", "comment": "\"Refactor Attention Scaling\"\n\nclass miniformer.layers.attention.MultiHeadSelfAttention\n\nRefactor the `forward` method of the `MultiHeadSelfAttention` class to make the scaling factor explicit.\n\n-[ Notes ]-\nThe attention formula `softmax(Q @ K.T / sqrt(d_k))` includes a scaling factor to prevent the dot products from growing too large and pushing the softmax into regions with extremely small gradients. Making this factor a local variable improves code clarity and maintainability. The end-to-end logic must remain identical to ensure the existing tests in `test_integration.py` continue to pass.", "class_name": "miniformer.layers.attention.MultiHeadSelfAttention", "class_link": "miniformer/layers/attention.py", "test_file_path": "tests/test_integration.py", "task_id": "miniformer-06"}
{"title": "FeedForward", "class_annotation": "miniformer.layers.feedforward.FeedForward", "comment": "\"Refactor FeedForward Layer with Dependency Injection\"\n\nclass miniformer.layers.feedforward.FeedForward\n\nRefactor the `FeedForward` class to accept an `activation_fn` object directly in its constructor, rather than a string name from the config.\n\n-[ Rationale ]-\nThis change decouples the `FeedForward` layer from the `config` object and the `activations` module, an example of Dependency Injection. It makes the layer more reusable and easier to test in isolation. You must also update the instantiation of `FeedForward` within the `TransformerBlock` in `miniformer/models/block.py` to pass the correct activation function object. The refactoring must be correct for all tests in `test_integration.py` to pass.", "class_name": "miniformer.layers.feedforward.FeedForward", "class_link": "miniformer/layers/feedforward.py", "test_file_path": "tests/test_integration.py", "task_id": "miniformer-07"}
{"title": "test_integration.py", "class_annotation": "tests.test_integration.test_integration.py", "comment": "\"Add Test for ReLU Activation\"\n\nfile tests/test_integration.py\n\nAdd a new test function `test_relu_activation` to the integration test suite.\n\n-[ Description ]-\nA robust test suite should cover various configurations. This test will ensure that the `TransformerBlock` can be successfully instantiated and executed using the 'relu' activation function, as specified in the configuration. The new test must verify that a forward pass completes without error and preserves the input tensor shape. The entire test suite must remain passing after your addition.", "class_name": "N/A", "class_link": "tests/test_integration.py", "test_file_path": "tests/test_integration.py", "task_id": "miniformer-08"}
{"title": "TransformerBlock", "class_annotation": "miniformer.models.block.TransformerBlock", "comment": "\"Implement Model Block Summary\"\n\nclass miniformer.models.block.TransformerBlock\n\nAdd a `summary(self)` method to the `TransformerBlock` class.\n\n-[ Description ]-\nThis method should provide a simple, human-readable string representation of the layers contained within the block, which is a common utility for debugging and inspecting model architectures. The returned string must contain the class names of the attention and MLP layers. This change is required to pass `test_block_summary_method` in `tests/test_models.py`.", "class_name": "miniformer.models.block.TransformerBlock", "class_link": "miniformer/models/block.py", "test_file_path": "tests/test_models.py", "task_id": "miniformer-09"}
{"title": "Miniformer", "class_annotation": "miniformer.models.model.Miniformer", "comment": "\"Create Full Miniformer Model\"\n\nclass miniformer.models.model.Miniformer(config)\n\nBases: `torch.nn.Module`\n\nCreate a new file `miniformer/models/model.py`. In it, define the main `Miniformer` class that assembles the complete model architecture.\n\n-[ Architecture ]-\nThis class orchestrates the entire forward pass of a decoder-only transformer. It combines token and positional embeddings, processes them through a stack of transformer blocks, and applies a final normalization step.\n\n__init__(self, config):\n  The constructor must initialize all the necessary sub-modules of the model.\n  Parameters:\n    * **config** (*TransformerConfig*): An object containing the model's hyperparameters.\n  Modules to create:\n    * **wte**: A `torch.nn.Embedding` for token embeddings. Its size should be `config.vocab_size` x `config.n_embd`.\n    * **wpe**: A `torch.nn.Embedding` for positional embeddings. Its size should be `config.block_size` x `config.n_embd`.\n    * **drop**: A `torch.nn.Dropout` layer, using the `config.resid_pdrop` probability.\n    * **h**: A `torch.nn.Sequential` container that holds `config.n_layer` instances of the `TransformerBlock` class.\n    * **ln_f**: A final `torch.nn.LayerNorm` layer.\n\nforward(self, idx):\n  Defines the computation performed at every call.\n  -[ Logic ]-\n  1. Get the token embeddings from `wte` using the input indices `idx`.\n  2. Get the positional embeddings from `wpe` for the sequence length of the input.\n  3. Add the token and positional embeddings together.\n  4. Apply dropout to the combined embeddings.\n  5. Pass the result through the stack of transformer blocks (`self.h`).\n  6. Apply the final layer norm (`self.ln_f`).\n  7. Return the final output tensor.\nThis implementation is required to pass `test_full_model_instantiation`.", "class_name": "miniformer.models.model.Miniformer", "class_link": "miniformer/models/model.py", "test_file_path": "tests/test_created_files.py", "task_id": "miniformer-10"}
{"title": "TransformerConfig", "class_annotation": "miniformer.config.TransformerConfig", "comment": "\"Verify Custom Configuration Instantiation\"\n\nclass miniformer.config.TransformerConfig\n\nThe test `test_config_instantiation` in `tests/test_config.py` validates that the `TransformerConfig` class correctly handles instantiation with custom, non-default values. Your task is to ensure the class implementation supports this. No changes should be necessary if the Pydantic model is correctly defined.", "class_name": "miniformer.config.TransformerConfig", "class_link": "miniformer/config.py", "test_file_path": "tests/test_config.py", "task_id": "miniformer-11"}
{"title": "CausalSelfAttention", "class_annotation": "miniformer.layers.attention.CausalSelfAttention", "comment": "\"Implement Flash Attention Placeholder\"\n\nclass miniformer.layers.attention.CausalSelfAttention\n\nModify the `CausalSelfAttention` and `TransformerConfig` classes to support a placeholder for Flash Attention.\n\n-[ Rationale ]-\nFlash Attention is a highly optimized algorithm for attention. While we will not implement it, we want our architecture to be forward-compatible. This involves adding a flag to the configuration and a check in the `forward` pass. This change is required to pass `test_flash_attention_placeholder`.\n\nImplementation Steps:\n1.  Add a `use_flash: bool = True` field to the `TransformerConfig` class.\n2.  In `CausalSelfAttention`, check for this flag in the `forward` method. If `True`, the method must raise a `NotImplementedError`.\n3.  The original attention logic should execute if the flag is `False`.", "class_name": "miniformer.layers.attention.CausalSelfAttention", "class_link": "miniformer/layers/attention.py", "test_file_path": "tests/test_layers.py", "task_id": "miniformer-12"}
{"title": "CausalSelfAttention", "class_annotation": "miniformer.layers.attention.CausalSelfAttention", "comment": "\"Expose Key-Value Projections\"\n\nclass miniformer.layers.attention.CausalSelfAttention\n\nRefactor the `c_attn` linear layer, which currently creates Q, K, and V projections in one batch, into three separate `nn.Linear` layers: `q_attn`, `k_attn`, and `v_attn`.\n\n-[ Notes ]-\nSeparating the projection matrices makes the architecture more explicit and is a prerequisite for more advanced schemes like Grouped-Query Attention (GQA). The forward pass must be updated to use these three distinct layers. The end-to-end functionality must remain identical to ensure all tests in `test_integration.py` continue to pass.", "class_name": "miniformer.layers.attention.CausalSelfAttention", "class_link": "miniformer/layers/attention.py", "test_file_path": "tests/test_integration.py", "task_id": "miniformer-13"}
{"title": "xavier_uniform_numpy", "class_annotation": "miniformer.utils.tensor_ops.xavier_uniform_numpy", "comment": "\"Add Xavier Uniform Initializer\"\n\nfunction miniformer.utils.tensor_ops.xavier_uniform_numpy(shape)\n\nIn `miniformer/utils/tensor_ops.py`, implement a new function `xavier_uniform_numpy(shape)` that provides a simplified, NumPy-based Xavier (or Glorot) uniform weight initializer.\n\n-[ Notes ]-\nThe Xavier initializer is designed to keep the variance of activations the same across every layer. The bound for the uniform distribution is calculated as `sqrt(6 / (fan_in + fan_out))`. For simplicity, you can assume a 2D weight matrix where `fan_in = shape[1]` and `fan_out = shape[0]`. This is required to pass `test_xavier_initializer`.", "class_name": "N/A", "class_link": "miniformer/utils/tensor_ops.py", "test_file_path": "tests/test_utils.py", "task_id": "miniformer-14"}
{"title": "LanguageModelHead", "class_annotation": "miniformer.models.head.LanguageModelHead", "comment": "\"Create Language Model Head\"\n\nclass miniformer.models.head.LanguageModelHead(config)\n\nBases: `torch.nn.Module`\n\nCreate a new file `miniformer/models/head.py` and define the `LanguageModelHead` class.\n\n-[ Description ]-\nThis is the final projection layer in a language model. It takes the high-level feature vectors produced by the transformer blocks and maps them to a score for each word in the vocabulary.\n\nImplementation:\n- It must inherit from `torch.nn.Module`.\n- The `__init__` method must create a single `torch.nn.Linear` layer named `lm_head`. This layer should map from the embedding dimension (`config.n_embd`) to the vocabulary size (`config.vocab_size`), with `bias=False`.\n- The `forward` method should accept a tensor and pass it through the `lm_head`.\nThis implementation is required to pass the `test_language_model_head`.", "class_name": "miniformer.models.head.LanguageModelHead", "class_link": "miniformer/models/head.py", "test_file_path": "tests/test_created_files.py", "task_id": "miniformer-15"}
