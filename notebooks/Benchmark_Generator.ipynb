{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmark Generator for the miniformer Code Agent Task\n",
        "\n",
        "This notebook orchestrates the complete generation of the `miniformer-15` benchmark, a high-fidelity, repo-level code generation task designed for evaluating LLM-based agents.\n",
        "\n",
        "### Core Objectives\n",
        "*   **Generate a Codebase:** Create the complete `miniformer` repository, a minimal but architecturally sound Python library for building transformers. The codebase is engineered to be statistically representative of real-world projects, featuring a long-tail file size distribution and a rich dependency graph.\n",
        "*   **Generate a Dataset:** Create a dataset of 15 programming tasks. These tasks are formatted as verbose, documentation-style prompts and are designed to be **verified by an executable test suite**, not simple string matching.\n",
        "*   **Produce Artifacts:** The final outputs of this notebook are:\n",
        "    1.  `miniformer_codebase.jsonl`: The complete codebase in JSONL format.\n",
        "    2.  `miniformer_tasks.jsonl`: The 15 tasks in the required JSONL format.\n",
        "    3.  `miniformer_repo.zip`: A downloadable archive of the physical repository.\n",
        "\n",
        "This setup is inspired by the principles outlined in the `CODEAGENT` paper, providing a rigorous and realistic environment for agent evaluation."
      ],
      "metadata": {
        "id": "p5Zz74PbNsw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup and Configuration\n",
        "\n",
        "This cell handles the initial setup for the generation process. It defines all necessary file paths and constants, and cleans up any artifacts from previous runs to ensure a clean slate."
      ],
      "metadata": {
        "id": "ewKdmFjFN0AH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlkZHqLla8f5",
        "outputId": "452900c8-c67e-4c6b-ebf2-d113a2d620a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Old artifacts removed.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "\n",
        "# --- Configuration ---\n",
        "# The root directory for the physical repository structure\n",
        "REPO_ROOT_PATH = \"/content/mini_transformers_repo\"\n",
        "\n",
        "# Output file paths for the JSONL benchmark data\n",
        "CODEBASE_OUTPUT_PATH = \"/content/mini_transformers_codebase.jsonl\"\n",
        "TASKS_OUTPUT_PATH = \"/content/mini_transformers_tasks.jsonl\"\n",
        "\n",
        "# Clean up previous runs to ensure a fresh start\n",
        "if os.path.exists(REPO_ROOT_PATH):\n",
        "    shutil.rmtree(REPO_ROOT_PATH)\n",
        "if os.path.exists(CODEBASE_OUTPUT_PATH):\n",
        "    os.remove(CODEBASE_OUTPUT_PATH)\n",
        "if os.path.exists(TASKS_OUTPUT_PATH):\n",
        "    os.remove(TASKS_OUTPUT_PATH)\n",
        "\n",
        "print(\"Setup complete. Old artifacts removed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Codebase Definition\n",
        "\n",
        "This cell defines the entire source code for the `miniformer` repository. The code is stored in a Python dictionary where keys are the relative file paths and values are the file contents.\n",
        "\n",
        "The codebase includes:\n",
        "*   The core `miniformer` library with sub-packages for `layers`, `models`, `utils`, etc.\n",
        "*   A comprehensive `tests/` directory containing the test suite required for the test-driven evaluation of our 15 tasks."
      ],
      "metadata": {
        "id": "NvT1Za4cN2Nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A dictionary holding all file paths and their content.\n",
        "# This version is engineered to have a statistical profile closer to the reference codebase.\n",
        "CODEBASE_CONTENT = {\n",
        "    # --- Project Root Files (Small) ---\n",
        "    \"main.py\": \"# Main entry point for agent tasks. Initially empty.\",\n",
        "    \"README.md\": \"\"\"\n",
        "# Miniformer\n",
        "A minimal, educational library for building transformer blocks, designed for agent-based code generation tasks.\n",
        "This library provides core components for building and experimenting with transformer architectures.\n",
        "\"\"\",\n",
        "    \"requirements.txt\": \"numpy\\ntorch\\nscipy\\npydantic\",\n",
        "\n",
        "    # --- Core Library: miniformer (Small __init__ files) ---\n",
        "    \"miniformer/__init__.py\": \"from .models.block import TransformerBlock\",\n",
        "    \"miniformer/config.py\": \"\"\"\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal\n",
        "\n",
        "class TransformerConfig(BaseModel):\n",
        "    \\\"\\\"\\\"Configuration for a Miniformer model.\\\"\\\"\\\"\n",
        "    vocab_size: int = Field(default=1000, ge=1)\n",
        "    n_layer: int = Field(default=4, ge=1)\n",
        "    n_head: int = Field(default=4, ge=1)\n",
        "    n_embd: int = Field(default=128, ge=1)\n",
        "    block_size: int = Field(default=256, ge=1)\n",
        "    attn_pdrop: float = Field(default=0.1, ge=0.0, le=1.0)\n",
        "    resid_pdrop: float = Field(default=0.1, ge=0.0, le=1.0)\n",
        "    activation_function: Literal['relu', 'gelu'] = 'gelu'\n",
        "\n",
        "    class Config:\n",
        "        validate_assignment = True\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.n_embd % self.n_head != 0:\n",
        "            raise ValueError(\"n_embd must be divisible by n_head\")\n",
        "\"\"\",\n",
        "\n",
        "    # --- Layers Package (Medium-sized files) ---\n",
        "    \"miniformer/layers/__init__.py\": \"from .attention import MultiHeadSelfAttention\",\n",
        "    \"miniformer/layers/attention.py\": \"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \\\"\\\"\\\"A vanilla multi-head masked self-attention layer with a projection at the end.\\\"\\\"\\\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # Key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # Output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # Regularization\n",
        "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
        "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
        "        # Causal mask\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                      .view(1, 1, config.block_size, config.block_size))\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # Batch size, sequence length, embedding dimensionality\n",
        "        # Calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # Causal self-attention\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # Re-assemble all head outputs\n",
        "        # Output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class MultiHeadSelfAttention(CausalSelfAttention):\n",
        "    \\\"\\\"\\\"Alias for CausalSelfAttention for clearer naming conventions.\\\"\\\"\\\"\n",
        "    pass\n",
        "\"\"\",\n",
        "    \"miniformer/layers/feedforward.py\": \"\"\"\n",
        "import torch.nn as nn\n",
        "from miniformer.activations import get_activation\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \\\"\\\"\\\"A position-wise feed-forward network.\\\"\\\"\\\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.dropout = nn.Dropout(config.resid_pdrop)\n",
        "        self.activation = get_activation(config.activation_function)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.c_proj(self.activation(self.c_fc(x))))\n",
        "\"\"\",\n",
        "\n",
        "    # --- Activations Module (Small) ---\n",
        "    \"miniformer/activations.py\": \"\"\"\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from scipy.special import erf\n",
        "import numpy as np\n",
        "\n",
        "def gelu_exact(x):\n",
        "    \\\"\\\"\\\"Gaussian Error Linear Unit (GELU) activation function using SciPy's erf for NumPy arrays.\\\"\\\"\\\"\n",
        "    return 0.5 * x * (1.0 + erf(x / np.sqrt(2.0)))\n",
        "\n",
        "def get_activation(name: str):\n",
        "    \\\"\\\"\\\"Returns the activation function corresponding to the name.\\\"\\\"\\\"\n",
        "    if name == 'relu':\n",
        "        return F.relu\n",
        "    elif name == 'gelu':\n",
        "        return F.gelu # PyTorch's optimized GELU for tensors\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown activation function: {name}\")\n",
        "\"\"\",\n",
        "\n",
        "    # --- Models Package (Small/Medium) ---\n",
        "    \"miniformer/models/__init__.py\": \"from .block import TransformerBlock\",\n",
        "    \"miniformer/models/block.py\": \"\"\"\n",
        "import torch.nn as nn\n",
        "from miniformer.layers.attention import MultiHeadSelfAttention\n",
        "from miniformer.layers.feedforward import FeedForward\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \\\"\\\"\\\"\n",
        "    A single block of a transformer model. It consists of a multi-head\n",
        "    self-attention layer followed by a feed-forward network. Layer\n",
        "    normalization and residual connections are applied.\n",
        "    \\\"\\\"\\\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = MultiHeadSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = FeedForward(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\"\"\",\n",
        "\n",
        "    # --- Utils Package (Small files) ---\n",
        "    \"miniformer/utils/__init__.py\": \"from .testing import assert_allclose\",\n",
        "    \"miniformer/utils/testing.py\": \"\"\"\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def assert_allclose(actual, desired, rtol=1e-6, atol=1e-6, label=\"\"):\n",
        "    \\\"\\\"\\\"A wrapper around np.testing.assert_allclose with a label for better test reporting.\\\"\\\"\\\"\n",
        "    try:\n",
        "        np.testing.assert_allclose(actual, desired, rtol=rtol, atol=atol)\n",
        "        if label: print(f\"Assertion PASSED for: {label}\")\n",
        "    except AssertionError as e:\n",
        "        print(f\"Assertion FAILED for: {label}\" if label else \"Assertion FAILED.\")\n",
        "        raise e\n",
        "\"\"\",\n",
        "    \"miniformer/utils/tensor_ops.py\": \"\"\"\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "\n",
        "def to_numpy(tensor):\n",
        "    \\\"\\\"\\\"Converts a PyTorch tensor to a NumPy array on the CPU.\\\"\\\"\\\"\n",
        "    if tensor is None: return None\n",
        "    return tensor.detach().cpu().numpy()\n",
        "\n",
        "def kaiming_uniform_numpy(shape, a=0, mode='fan_in', nonlinearity='leaky_relu'):\n",
        "    \\\"\\\"\\\"A simplified numpy-based Kaiming uniform initializer.\\\"\\\"\\\"\n",
        "    fan = np.prod(shape[1:]) if mode == 'fan_in' else shape[0]\n",
        "    gain = math.sqrt(2.0 / (1 + a**2)) if nonlinearity == 'leaky_relu' else 1.0\n",
        "    std = gain / math.sqrt(fan)\n",
        "    bound = math.sqrt(3.0) * std\n",
        "    return np.random.uniform(-bound, bound, size=shape)\n",
        "\"\"\",\n",
        "\n",
        "    # --- Tests Directory (Our single \"heavyweight\" file) ---\n",
        "    \"tests/__init__.py\": \"\",\n",
        "    \"tests/test_integration.py\": \"\"\"\n",
        "# ==============================================================================\n",
        "# Comprehensive Integration Test Suite for the Miniformer Library\n",
        "# ==============================================================================\n",
        "#\n",
        "# This file serves as the primary validation mechanism for the entire miniformer\n",
        "# codebase. Unlike unit tests that might focus on a single function, these\n",
        "# integration tests ensure that all the different components (config, layers,\n",
        "# models, utils) work together as expected.\n",
        "#\n",
        "# This file is intentionally verbose and detailed to:\n",
        "# 1. Provide a clear example of how to use the library's components.\n",
        "# 2. Serve as a challenging, realistic file for a code generation agent to modify.\n",
        "# 3. Create a statistical outlier in file size, mimicking real-world codebases.\n",
        "#\n",
        "# It imports from nearly every other module in the project.\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "import pytest # Using pytest for better test structure and fixtures\n",
        "\n",
        "# Import all major components from the library\n",
        "from miniformer.config import TransformerConfig\n",
        "from miniformer.models.block import TransformerBlock\n",
        "from miniformer.layers.attention import MultiHeadSelfAttention\n",
        "from miniformer.layers.feedforward import FeedForward\n",
        "from miniformer.activations import get_activation\n",
        "from miniformer.utils.testing import assert_allclose\n",
        "from miniformer.utils.tensor_ops import to_numpy\n",
        "\n",
        "# --- Test Fixtures ---\n",
        "\n",
        "@pytest.fixture(scope=\"module\")\n",
        "def default_config():\n",
        "    \\\"\\\"\\\"Provides a default, consistent TransformerConfig for all tests in this module.\\\"\\\"\\\"\n",
        "    return TransformerConfig(\n",
        "        vocab_size=100,\n",
        "        n_layer=2,\n",
        "        n_head=4,\n",
        "        n_embd=64,\n",
        "        block_size=128,\n",
        "        attn_pdrop=0.0,  # Disable dropout for deterministic tests\n",
        "        resid_pdrop=0.0\n",
        "    )\n",
        "\n",
        "@pytest.fixture\n",
        "def transformer_block(default_config):\n",
        "    \\\"\\\"\\\"Provides an initialized TransformerBlock instance for testing.\\\"\\\"\\\"\n",
        "    model = TransformerBlock(default_config)\n",
        "    model.eval() # Set to evaluation mode to disable dropout behavior\n",
        "    return model\n",
        "\n",
        "\n",
        "# --- Core Functionality Tests ---\n",
        "\n",
        "def test_block_forward_pass_shape(transformer_block, default_config):\n",
        "    \\\"\\\"\\\"\n",
        "    PURPOSE: To verify that a forward pass through a TransformerBlock preserves\n",
        "    the shape of the input tensor. This is the most fundamental sanity check.\n",
        "    If input shape is (B, T, C), output shape must also be (B, T, C).\n",
        "    \\\"\\\"\\\"\n",
        "    print(\"\\\\n--- Running Test: Block Forward Pass Shape ---\")\n",
        "    batch_size = 4\n",
        "    seq_len = 32\n",
        "    input_tensor = torch.rand(batch_size, seq_len, default_config.n_embd)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_tensor = transformer_block(input_tensor)\n",
        "\n",
        "    assert input_tensor.shape == output_tensor.shape, \\\\\n",
        "        f\"Shape mismatch: In {input_tensor.shape} vs Out {output_tensor.shape}\"\n",
        "    print(\"Shape preservation test PASSED.\")\n",
        "\n",
        "\n",
        "def test_attention_causality(default_config):\n",
        "    \\\"\\\"\\\"\n",
        "    PURPOSE: To rigorously test the causal nature of the self-attention mechanism.\n",
        "    A token at position `i` should NEVER be influenced by tokens at positions `j > i`.\n",
        "\n",
        "    METHOD:\n",
        "    1. Create an input tensor `input1`.\n",
        "    2. Create a second tensor `input2` which is identical to `input1` initially.\n",
        "    3. Modify `input2` at a future position (e.g., add noise at `t+1`).\n",
        "    4. Pass both tensors through the attention layer.\n",
        "    5. The output for position `t` should be IDENTICAL for both `output1` and `output2`.\n",
        "    \\\"\\\"\\\"\n",
        "    print(\"\\\\n--- Running Test: Attention Causality ---\")\n",
        "    attention_layer = MultiHeadSelfAttention(default_config).eval()\n",
        "    seq_len = 16\n",
        "    pos_to_check = 8 # The position we will observe\n",
        "\n",
        "    # Create two input tensors.\n",
        "    input1 = torch.randn(1, seq_len, default_config.n_embd)\n",
        "    input2 = input1.clone()\n",
        "    input2[:, pos_to_check + 1, :] += 10.0 # Add large noise to a future token\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output1 = attention_layer(input1)\n",
        "        output2 = attention_layer(input2)\n",
        "\n",
        "    # The output up to and including `pos_to_check` should be bit-for-bit identical.\n",
        "    out1_numpy = to_numpy(output1[:, :pos_to_check + 1, :])\n",
        "    out2_numpy = to_numpy(output2[:, :pos_to_check + 1, :])\n",
        "\n",
        "    assert_allclose(out1_numpy, out2_numpy, label=\"Causality Check\")\n",
        "    print(\"Causality test PASSED.\")\n",
        "\n",
        "def test_batch_independence(transformer_block):\n",
        "    \\\"\\\"\\\"\n",
        "    PURPOSE: To ensure that computations for different items in a batch are\n",
        "    completely independent of one another.\n",
        "\n",
        "    METHOD:\n",
        "    1. Process a full batch of size N > 1.\n",
        "    2. Process the first item of that batch alone (batch size 1).\n",
        "    3. The output from the single-item pass must be identical to the first slice\n",
        "       of the output from the full-batch pass.\n",
        "    \\\"\\\"\\\"\n",
        "    print(\"\\\\n--- Running Test: Batch Independence ---\")\n",
        "    # Input with batch size > 1\n",
        "    full_batch_input = torch.rand(4, 16, transformer_block.attn.n_embd)\n",
        "    # Input with only the first element of the batch\n",
        "    single_item_input = full_batch_input[0:1, :, :].clone()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        full_batch_output = transformer_block(full_batch_input)\n",
        "        single_item_output = transformer_block(single_item_input)\n",
        "\n",
        "    # Compare the first item from the full batch output to the single item output\n",
        "    out_full_numpy = to_numpy(full_batch_output[0:1, :, :])\n",
        "    out_single_numpy = to_numpy(single_item_output)\n",
        "\n",
        "    assert_allclose(out_full_numpy, out_single_numpy, label=\"Batch Independence\")\n",
        "    print(\"Batch independence test PASSED.\")\n",
        "\n",
        "# --- A placeholder for future tests ---\n",
        "def test_model_training_step():\n",
        "    \\\"\\\"\\\"\n",
        "    PURPOSE: This is a placeholder test. A real implementation would check if the\n",
        "    model parameters are updated after a backward pass and optimizer step.\n",
        "    For this benchmark, we just confirm that it runs without error.\n",
        "    \\\"\\\"\\\"\n",
        "    print(\"\\\\n--- Running Test: Model Training Step (Placeholder) ---\")\n",
        "    assert True\n",
        "    print(\"Training step placeholder test PASSED.\")\n",
        "\n",
        "# --- Main execution block to run tests if the file is executed directly ---\n",
        "\n",
        "def run_all_tests():\n",
        "    \\\"\\\"\\\"Main function to run all defined tests sequentially.\\\"\\\"\\\"\n",
        "    # This function is more for direct execution than for pytest\n",
        "    config = default_config()\n",
        "    block = transformer_block(config)\n",
        "\n",
        "    test_block_forward_pass_shape(block, config)\n",
        "    test_attention_causality(config)\n",
        "    test_batch_independence(block)\n",
        "    test_model_training_step()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=============================================\")\n",
        "    print(\"    RUNNING MINIFORMER INTEGRATION SUITE     \")\n",
        "    print(\"=============================================\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Manually run tests if not using pytest\n",
        "    run_all_tests()\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(\"\\\\n=============================================\")\n",
        "    print(f\"   SUITE FINISHED in {end_time - start_time:.2f}s\")\n",
        "    print(\"=============================================\")\n",
        "\"\"\",\n",
        "\"tests/test_config.py\": \"\"\"\n",
        "import pytest\n",
        "from miniformer.config import TransformerConfig\n",
        "\n",
        "def test_config_instantiation():\n",
        "    # Tests that a config can be created with custom values (Task #11)\n",
        "    config = TransformerConfig(n_layer=2, n_head=2, n_embd=32)\n",
        "    assert config.n_layer == 2\n",
        "    assert config.n_head == 2\n",
        "    assert config.n_embd == 32\n",
        "\n",
        "def test_config_to_dict_method():\n",
        "    # This test is expected to fail until Task #3 is completed.\n",
        "    config = TransformerConfig()\n",
        "    try:\n",
        "        d = config.to_dict()\n",
        "        assert isinstance(d, dict)\n",
        "        assert d['n_embd'] == 128\n",
        "    except AttributeError:\n",
        "        pytest.fail(\"The 'to_dict' method does not exist on TransformerConfig.\")\n",
        "\n",
        "def test_config_bias_field():\n",
        "    # This test is expected to fail until Task #1 is completed.\n",
        "    try:\n",
        "        config = TransformerConfig(use_bias=True)\n",
        "        assert config.use_bias is True\n",
        "    except TypeError:\n",
        "        pytest.fail(\"The 'use_bias' field does not exist on TransformerConfig.\")\n",
        "\"\"\",\n",
        "\n",
        "\"tests/test_activations.py\": \"\"\"\n",
        "import pytest\n",
        "import torch\n",
        "from miniformer.activations import get_activation\n",
        "\n",
        "def test_swish_activation():\n",
        "    # This test is expected to fail until Task #4 is completed.\n",
        "    try:\n",
        "        swish = get_activation('swish')\n",
        "        x = torch.tensor([1.0, 2.0, -1.0])\n",
        "        expected = x * torch.sigmoid(x)\n",
        "        assert torch.allclose(swish(x), expected)\n",
        "    except ValueError:\n",
        "        pytest.fail(\"Activation 'swish' is not registered in get_activation.\")\n",
        "\"\"\",\n",
        "\n",
        "\"tests/test_layers.py\": \"\"\"\n",
        "import pytest\n",
        "import torch\n",
        "from miniformer.config import TransformerConfig\n",
        "\n",
        "def test_flash_attention_placeholder():\n",
        "    # This test is expected to fail until Task #12 is completed.\n",
        "    from miniformer.layers.attention import CausalSelfAttention\n",
        "\n",
        "    # This config would enable flash attention if the field existed\n",
        "    try:\n",
        "        config = TransformerConfig(use_flash=True, n_embd=32, n_head=4)\n",
        "        attention = CausalSelfAttention(config)\n",
        "        input_tensor = torch.rand(1, 16, 32)\n",
        "        with pytest.raises(NotImplementedError, match=\"Flash Attention not yet implemented\"):\n",
        "            attention(input_tensor)\n",
        "    except (TypeError, AttributeError):\n",
        "        pytest.fail(\"Task #12 is not complete. Either 'use_flash' field is missing or the check in CausalSelfAttention is not implemented.\")\n",
        "\"\"\",\n",
        "\n",
        "\"tests/test_models.py\": \"\"\"\n",
        "import pytest\n",
        "from miniformer.config import TransformerConfig\n",
        "from miniformer.models.block import TransformerBlock\n",
        "\n",
        "def test_block_summary_method():\n",
        "    # This test is expected to fail until Task #9 is completed.\n",
        "    config = TransformerConfig()\n",
        "    block = TransformerBlock(config)\n",
        "    try:\n",
        "        # Check if the method exists and returns a string containing layer names\n",
        "        summary_str = block.summary()\n",
        "        assert isinstance(summary_str, str)\n",
        "        assert \"MultiHeadSelfAttention\" in summary_str\n",
        "        assert \"FeedForward\" in summary_str\n",
        "    except (AttributeError, TypeError):\n",
        "        pytest.fail(\"The 'summary' method does not exist or does not return a string.\")\n",
        "\"\"\",\n",
        "\n",
        "\"tests/test_utils.py\": \"\"\"\n",
        "import torch\n",
        "import numpy as np\n",
        "from miniformer.utils.tensor_ops import to_numpy\n",
        "\n",
        "def test_to_numpy_conversion():\n",
        "    # Tests the functionality for Task #2\n",
        "    tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
        "    arr = to_numpy(tensor)\n",
        "    assert isinstance(arr, np.ndarray)\n",
        "    assert arr.shape == (2, 2)\n",
        "\n",
        "def test_xavier_initializer():\n",
        "    # This test is expected to fail until Task #14 (the replacement task) is completed.\n",
        "    try:\n",
        "        from miniformer.utils.tensor_ops import xavier_uniform_numpy\n",
        "        shape = (100, 100)\n",
        "        result = xavier_uniform_numpy(shape)\n",
        "        assert result.shape == shape\n",
        "        # Check if values are within a reasonable bound for uniform dist\n",
        "        assert np.abs(result).mean() < 0.5\n",
        "    except ImportError:\n",
        "        pytest.fail(\"Function 'xavier_uniform_numpy' not found in tensor_ops.py.\")\n",
        "\"\"\",\n",
        "\n",
        "\"tests/test_created_files.py\": \"\"\"\n",
        "import pytest\n",
        "from miniformer.config import TransformerConfig\n",
        "\n",
        "# This file contains tests for components that are created by agent tasks.\n",
        "# These tests are EXPECTED TO FAIL until the agent completes the tasks.\n",
        "\n",
        "def test_positional_embedding_layer():\n",
        "    # This test is for Task #5.\n",
        "    try:\n",
        "        from miniformer.layers.embedding import PositionalEmbedding\n",
        "        config = TransformerConfig(block_size=64, n_embd=32)\n",
        "        layer = PositionalEmbedding(config)\n",
        "        input_tensor = torch.rand(2, 16, 32) # B, T, C\n",
        "        pos_emb = layer(input_tensor)\n",
        "        assert pos_emb.shape == (16, 32) # T, C\n",
        "    except ImportError:\n",
        "        pytest.fail(\"File 'miniformer/layers/embedding.py' or class 'PositionalEmbedding' not found.\")\n",
        "\n",
        "def test_full_model_instantiation():\n",
        "    # This test is for Task #10.\n",
        "    try:\n",
        "        from miniformer.models.model import Miniformer\n",
        "        config = TransformerConfig()\n",
        "        model = Miniformer(config)\n",
        "        assert model is not None\n",
        "    except ImportError:\n",
        "        pytest.fail(\"File 'miniformer/models/model.py' or class 'Miniformer' not found.\")\n",
        "\n",
        "def test_language_model_head():\n",
        "    # This test is for Task #15.\n",
        "    try:\n",
        "        from miniformer.models.head import LanguageModelHead\n",
        "        config = TransformerConfig(n_embd=32, vocab_size=100)\n",
        "        head = LanguageModelHead(config)\n",
        "        input_tensor = torch.rand(2, 16, 32) # B, T, C\n",
        "        logits = head(input_tensor)\n",
        "        assert logits.shape == (2, 16, 100) # B, T, vocab_size\n",
        "    except ImportError:\n",
        "        pytest.fail(\"File 'miniformer/models/head.py' or class 'LanguageModelHead' not found.\")\n",
        "\"\"\"\n",
        "}"
      ],
      "metadata": {
        "id": "ESHdRAW1beTr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Task Dataset Definition\n",
        "\n",
        "This cell defines the 15 programming tasks for our benchmark. These are the \"raw\" task definitions, written in a human-readable format.\n",
        "\n",
        "Key characteristics of these tasks:\n",
        "*   **Test-Driven:** Each task is designed to be verified by making a specific test in the `tests/` directory pass.\n",
        "*   **Rich Prompts:** The `prompt` for each task is written in a verbose, documentation-style format to mimic real-world specifications.\n",
        "*   **Varied Difficulty:** The tasks range from simple modifications to complex refactoring and file creation."
      ],
      "metadata": {
        "id": "ThqBRxilN4TJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Raw, human-centric task definitions with verbose, documentation-style prompts.\n",
        "# This version is designed to match the content style of the reference benchmark.\n",
        "TASK_DEFINITIONS = [\n",
        "    {\n",
        "        \"task_id\": \"miniformer-01\",\n",
        "        \"target_symbol\": \"TransformerConfig\",\n",
        "        \"target_file\": \"miniformer/config.py\",\n",
        "        \"test_file_path\": \"tests/test_config.py\",\n",
        "        \"prompt\": \"\"\"\n",
        "\"Add Bias Configuration to TransformerConfig\"\n",
        "\n",
        "class miniformer.config.TransformerConfig\n",
        "\n",
        "Modify the configuration class to support toggling bias terms in linear layers.\n",
        "\n",
        "-[ Notes ]-\n",
        "In some transformer architectures, bias terms in linear projections are considered redundant, especially when followed by a normalization layer like LayerNorm. Providing a configurable flag for this is a common design pattern for architectural experimentation. Your change is required to make the test `test_config_bias_field` in `tests/test_config.py` pass.\n",
        "\n",
        "Parameters to Add:\n",
        "  * **use_bias** (*bool*) -- If True, linear layers will include a bias term. This should default to `False` to follow modern best practices.\n",
        "\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"task_id\": \"miniformer-02\",\n",
        "        \"target_symbol\": \"to_numpy\",\n",
        "        \"target_file\": \"miniformer/utils/tensor_ops.py\",\n",
        "        \"test_file_path\": \"tests/test_utils.py\",\n",
        "        \"prompt\": \"\"\"\n",
        "\"Verify Tensor Conversion Utility\"\n",
        "\n",
        "function miniformer.utils.tensor_ops.to_numpy(tensor)\n",
        "\n",
        "Verify that the `to_numpy` utility function is correctly implemented.\n",
        "\n",
        "-[ Description ]-\n",
        "This function serves as a standard bridge between PyTorch tensors and NumPy arrays, a common operation in ML workflows for debugging, analysis, or interfacing with other libraries. It should handle the device transfer (`.cpu()`) and gradient detachment (`.detach()`) before conversion. The test `test_to_numpy_conversion` in `tests/test_utils.py` validates this behavior. No changes are needed if the implementation is correct.\n",
        "\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"task_id\": \"miniformer-03\",\n",
        "        \"target_symbol\": \"TransformerConfig\",\n",
        "        \"target_file\": \"miniformer/config.py\",\n",
        "        \"test_file_path\": \"tests/test_config.py\",\n",
        "        \"prompt\": \"\"\"\n",
        "\"Implement Configuration Export Method\"\n",
        "\n",
        "class miniformer.config.TransformerConfig\n",
        "\n",
        "Add a method to the `TransformerConfig` class for exporting its settings.\n",
        "\n",
        "Method to Add:\n",
        "  to_dict(self)\n",
        "    Exports the configuration instance to a Python dictionary. This is crucial for serialization (e.g., saving to JSON), logging experiment parameters, or re-instantiating models from a saved state. The implementation should leverage the built-in `.dict()` method from the Pydantic `BaseModel`. This is required to pass the `test_config_to_dict_method`.\n",
        "\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"task_id\": \"miniformer-04\",\n",
        "        \"target_symbol\": \"get_activation\",\n",
        "        \"target_file\": \"miniformer/activations.py\",\n",
        "        \"test_file_path\": \"tests/test_activations.py\",\n",
        "        \"prompt\": \"\"\"\n",
        "\"Add Swish Activation Support\"\n",
        "\n",
        "function miniformer.activations.get_activation(name)\n",
        "\n",
        "Extend the activation function factory to include support for 'swish'.\n",
        "\n",
        "-[ Notes ]-\n",
        "The Swish activation function, defined as `f(x) = x * sigmoid(x)`, is a smooth, non-monotonic function that often matches or exceeds the performance of ReLU on deeper models.\n",
        "\n",
        "Implementation Steps:\n",
        "1.  Implement a new Python function `swish(x)` that computes the activation. It should use `torch.sigmoid`.\n",
        "2.  Modify the `get_activation` function to return a reference to your `swish` function when the input `name` is 'swish'.\n",
        "This change is required to pass `test_swish_activation` in `tests/test_activations.py`.\n",
        "\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"task_id\": \"miniformer-05\",\n",
        "        \"target_symbol\": \"PositionalEmbedding\",\n",
        "        \"target_file\": \"miniformer/layers/embedding.py\",\n",
        "        \"test_file_path\": \"tests/test_created_files.py\",\n",
        "        \"prompt\": \"\"\"\n",
        "\"Create PositionalEmbedding Layer\"\n",
        "\n",
        "class miniformer.layers.embedding.PositionalEmbedding(config)\n",
        "\n",
        "Bases: `torch.nn.Module`\n",
        "\n",
        "Create a new file `miniformer/layers/embedding.py`. In it, define a `PositionalEmbedding` class.\n",
        "\n",
        "-[ Description ]-\n",
        "This layer learns a unique vector for each position in the input sequence, up to a maximum length defined by `config.block_size`. These embeddings are added to token embeddings to provide the model with information about token order, which is essential for sequence processing tasks since self-attention is permutation-invariant. This implementation is required to pass `test_positional_embedding_layer`.\n",
        "\n",
        "__init__(self, config):\n",
        "  The constructor must initialize a `torch.nn.Embedding` layer.\n",
        "  Parameters:\n",
        "    * **num_embeddings** (*int*): The maximum number of positions, from `config.block_size`.\n",
        "    * **embedding_dim** (*int*): The dimensionality of the embedding vectors, from `config.n_embd`.\n",
        "\n",
        "forward(self, x):\n",
        "  The forward pass must generate the positional embeddings for the sequence length `T` of the input tensor `x`.\n",
        "  Parameters:\n",
        "    * **x** (*torch.Tensor* of shape *(B, T, C)*): The input tensor from the previous layer. Only the sequence length `T` is used.\n",
        "  Returns:\n",
        "    * **pos_emb** (*torch.Tensor* of shape *(T, C)*): The learned positional embeddings for positions 0 to T-1.\n",
        "\"\"\"\n",
        "    },\n",
        "    # --- [Tasks 6-9 follow a similar, moderately verbose pattern] ---\n",
        "    {\n",
        "        \"task_id\": \"miniformer-06\", \"target_symbol\": \"MultiHeadSelfAttention\", \"target_file\": \"miniformer/layers/attention.py\", \"test_file_path\": \"tests/test_integration.py\",\n",
        "        \"prompt\": \"\"\"\n",
        "\"Refactor Attention Scaling\"\n",
        "\n",
        "class miniformer.layers.attention.MultiHeadSelfAttention\n",
        "\n",
        "Refactor the `forward` method of the `MultiHeadSelfAttention` class to make the scaling factor explicit.\n",
        "\n",
        "-[ Notes ]-\n",
        "The attention formula `softmax(Q @ K.T / sqrt(d_k))` includes a scaling factor to prevent the dot products from growing too large and pushing the softmax into regions with extremely small gradients. Making this factor a local variable improves code clarity and maintainability. The end-to-end logic must remain identical to ensure the existing tests in `test_integration.py` continue to pass.\n",
        "\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"task_id\": \"miniformer-07\", \"target_symbol\": \"FeedForward\", \"target_file\": \"miniformer/layers/feedforward.py\", \"test_file_path\": \"tests/test_integration.py\",\n",
        "        \"prompt\": \"\"\"\n",
        "\"Refactor FeedForward Layer with Dependency Injection\"\n",
        "\n",
        "class miniformer.layers.feedforward.FeedForward\n",
        "\n",
        "Refactor the `FeedForward` class to accept an `activation_fn` object directly in its constructor, rather than a string name from the config.\n",
        "\n",
        "-[ Rationale ]-\n",
        "This change decouples the `FeedForward` layer from the `config` object and the `activations` module, an example of Dependency Injection. It makes the layer more reusable and easier to test in isolation. You must also update the instantiation of `FeedForward` within the `TransformerBlock` in `miniformer/models/block.py` to pass the correct activation function object. The refactoring must be correct for all tests in `test_integration.py` to pass.\n",
        "\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"task_id\": \"miniformer-08\", \"target_symbol\": \"test_integration.py\", \"target_file\": \"tests/test_integration.py\", \"test_file_path\": \"tests/test_integration.py\",\n",
        "        \"prompt\": \"\"\"\n",
        "\"Add Test for ReLU Activation\"\n",
        "\n",
        "file tests/test_integration.py\n",
        "\n",
        "Add a new test function `test_relu_activation` to the integration test suite.\n",
        "\n",
        "-[ Description ]-\n",
        "A robust test suite should cover various configurations. This test will ensure that the `TransformerBlock` can be successfully instantiated and executed using the 'relu' activation function, as specified in the configuration. The new test must verify that a forward pass completes without error and preserves the input tensor shape. The entire test suite must remain passing after your addition.\n",
        "\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"task_id\": \"miniformer-09\", \"target_symbol\": \"TransformerBlock\", \"target_file\": \"miniformer/models/block.py\", \"test_file_path\": \"tests/test_models.py\",\n",
        "        \"prompt\": \"\"\"\n",
        "\"Implement Model Block Summary\"\n",
        "\n",
        "class miniformer.models.block.TransformerBlock\n",
        "\n",
        "Add a `summary(self)` method to the `TransformerBlock` class.\n",
        "\n",
        "-[ Description ]-\n",
        "This method should provide a simple, human-readable string representation of the layers contained within the block, which is a common utility for debugging and inspecting model architectures. The returned string must contain the class names of the attention and MLP layers. This change is required to pass `test_block_summary_method` in `tests/test_models.py`.\n",
        "\"\"\"\n",
        "    },\n",
        "    # --- [Task 10 is our \"heavyweight\" task, mimicking the reference's verbosity] ---\n",
        "    {\n",
        "        \"task_id\": \"miniformer-10\",\n",
        "        \"target_symbol\": \"Miniformer\",\n",
        "        \"target_file\": \"miniformer/models/model.py\",\n",
        "        \"test_file_path\": \"tests/test_created_files.py\",\n",
        "        \"prompt\": \"\"\"\n",
        "\"Create Full Miniformer Model\"\n",
        "\n",
        "class miniformer.models.model.Miniformer(config)\n",
        "\n",
        "Bases: `torch.nn.Module`\n",
        "\n",
        "Create a new file `miniformer/models/model.py`. In it, define the main `Miniformer` class that assembles the complete model architecture.\n",
        "\n",
        "-[ Architecture ]-\n",
        "This class orchestrates the entire forward pass of a decoder-only transformer. It combines token and positional embeddings, processes them through a stack of transformer blocks, and applies a final normalization step.\n",
        "\n",
        "__init__(self, config):\n",
        "  The constructor must initialize all the necessary sub-modules of the model.\n",
        "  Parameters:\n",
        "    * **config** (*TransformerConfig*): An object containing the model's hyperparameters.\n",
        "  Modules to create:\n",
        "    * **wte**: A `torch.nn.Embedding` for token embeddings. Its size should be `config.vocab_size` x `config.n_embd`.\n",
        "    * **wpe**: A `torch.nn.Embedding` for positional embeddings. Its size should be `config.block_size` x `config.n_embd`.\n",
        "    * **drop**: A `torch.nn.Dropout` layer, using the `config.resid_pdrop` probability.\n",
        "    * **h**: A `torch.nn.Sequential` container that holds `config.n_layer` instances of the `TransformerBlock` class.\n",
        "    * **ln_f**: A final `torch.nn.LayerNorm` layer.\n",
        "\n",
        "forward(self, idx):\n",
        "  Defines the computation performed at every call.\n",
        "  -[ Logic ]-\n",
        "  1. Get the token embeddings from `wte` using the input indices `idx`.\n",
        "  2. Get the positional embeddings from `wpe` for the sequence length of the input.\n",
        "  3. Add the token and positional embeddings together.\n",
        "  4. Apply dropout to the combined embeddings.\n",
        "  5. Pass the result through the stack of transformer blocks (`self.h`).\n",
        "  6. Apply the final layer norm (`self.ln_f`).\n",
        "  7. Return the final output tensor.\n",
        "This implementation is required to pass `test_full_model_instantiation`.\n",
        "\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"task_id\": \"miniformer-11\", \"target_symbol\": \"TransformerConfig\", \"target_file\": \"miniformer/config.py\", \"test_file_path\": \"tests/test_config.py\",\n",
        "        \"prompt\": \"\"\"\n",
        "\"Verify Custom Configuration Instantiation\"\n",
        "\n",
        "class miniformer.config.TransformerConfig\n",
        "\n",
        "The test `test_config_instantiation` in `tests/test_config.py` validates that the `TransformerConfig` class correctly handles instantiation with custom, non-default values. Your task is to ensure the class implementation supports this. No changes should be necessary if the Pydantic model is correctly defined.\n",
        "\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"task_id\": \"miniformer-12\", \"target_symbol\": \"CausalSelfAttention\", \"target_file\": \"miniformer/layers/attention.py\", \"test_file_path\": \"tests/test_layers.py\",\n",
        "        \"prompt\": \"\"\"\n",
        "\"Implement Flash Attention Placeholder\"\n",
        "\n",
        "class miniformer.layers.attention.CausalSelfAttention\n",
        "\n",
        "Modify the `CausalSelfAttention` and `TransformerConfig` classes to support a placeholder for Flash Attention.\n",
        "\n",
        "-[ Rationale ]-\n",
        "Flash Attention is a highly optimized algorithm for attention. While we will not implement it, we want our architecture to be forward-compatible. This involves adding a flag to the configuration and a check in the `forward` pass. This change is required to pass `test_flash_attention_placeholder`.\n",
        "\n",
        "Implementation Steps:\n",
        "1.  Add a `use_flash: bool = True` field to the `TransformerConfig` class.\n",
        "2.  In `CausalSelfAttention`, check for this flag in the `forward` method. If `True`, the method must raise a `NotImplementedError`.\n",
        "3.  The original attention logic should execute if the flag is `False`.\n",
        "\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"task_id\": \"miniformer-13\", \"target_symbol\": \"CausalSelfAttention\", \"target_file\": \"miniformer/layers/attention.py\", \"test_file_path\": \"tests/test_integration.py\",\n",
        "        \"prompt\": \"\"\"\n",
        "\"Expose Key-Value Projections\"\n",
        "\n",
        "class miniformer.layers.attention.CausalSelfAttention\n",
        "\n",
        "Refactor the `c_attn` linear layer, which currently creates Q, K, and V projections in one batch, into three separate `nn.Linear` layers: `q_attn`, `k_attn`, and `v_attn`.\n",
        "\n",
        "-[ Notes ]-\n",
        "Separating the projection matrices makes the architecture more explicit and is a prerequisite for more advanced schemes like Grouped-Query Attention (GQA). The forward pass must be updated to use these three distinct layers. The end-to-end functionality must remain identical to ensure all tests in `test_integration.py` continue to pass.\n",
        "\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"task_id\": \"miniformer-14\", \"target_symbol\": \"xavier_uniform_numpy\", \"target_file\": \"miniformer/utils/tensor_ops.py\", \"test_file_path\": \"tests/test_utils.py\",\n",
        "        \"prompt\": \"\"\"\n",
        "\"Add Xavier Uniform Initializer\"\n",
        "\n",
        "function miniformer.utils.tensor_ops.xavier_uniform_numpy(shape)\n",
        "\n",
        "In `miniformer/utils/tensor_ops.py`, implement a new function `xavier_uniform_numpy(shape)` that provides a simplified, NumPy-based Xavier (or Glorot) uniform weight initializer.\n",
        "\n",
        "-[ Notes ]-\n",
        "The Xavier initializer is designed to keep the variance of activations the same across every layer. The bound for the uniform distribution is calculated as `sqrt(6 / (fan_in + fan_out))`. For simplicity, you can assume a 2D weight matrix where `fan_in = shape[1]` and `fan_out = shape[0]`. This is required to pass `test_xavier_initializer`.\n",
        "\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"task_id\": \"miniformer-15\", \"target_symbol\": \"LanguageModelHead\", \"target_file\": \"miniformer/models/head.py\", \"test_file_path\": \"tests/test_created_files.py\",\n",
        "        \"prompt\": \"\"\"\n",
        "\"Create Language Model Head\"\n",
        "\n",
        "class miniformer.models.head.LanguageModelHead(config)\n",
        "\n",
        "Bases: `torch.nn.Module`\n",
        "\n",
        "Create a new file `miniformer/models/head.py` and define the `LanguageModelHead` class.\n",
        "\n",
        "-[ Description ]-\n",
        "This is the final projection layer in a language model. It takes the high-level feature vectors produced by the transformer blocks and maps them to a score for each word in the vocabulary.\n",
        "\n",
        "Implementation:\n",
        "- It must inherit from `torch.nn.Module`.\n",
        "- The `__init__` method must create a single `torch.nn.Linear` layer named `lm_head`. This layer should map from the embedding dimension (`config.n_embd`) to the vocabulary size (`config.vocab_size`), with `bias=False`.\n",
        "- The `forward` method should accept a tensor and pass it through the `lm_head`.\n",
        "This implementation is required to pass the `test_language_model_head`.\n",
        "\"\"\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "nM_z6PawbqB4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Task Enrichment and Formatting\n",
        "\n",
        "This cell contains the `enrich_task_definitions` function. Its purpose is to process the raw task list from the previous cell and transform it into the final, precise JSONL schema required by our benchmark.\n",
        "\n",
        "It uses Python's Abstract Syntax Tree (`ast`) module to introspect the codebase and intelligently generate metadata fields like `class_annotation` and `class_name`, ensuring the final output matches the structure of the reference `CODEAGENTBENCH` dataset."
      ],
      "metadata": {
        "id": "XxKLT4jhN5rF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "def enrich_task_definitions(task_list, codebase_content):\n",
        "    \"\"\"\n",
        "    Introspects the codebase to find authentic metadata for each task,\n",
        "    then builds the final task records with the exact schema from the reference,\n",
        "    including the 'title' field. This version relies on the `test_file_path`\n",
        "    for evaluation.\n",
        "    \"\"\"\n",
        "    enriched_tasks = []\n",
        "\n",
        "    # Build an AST for every Python file to aid in metadata generation\n",
        "    file_asts = {}\n",
        "    for path, content in codebase_content.items():\n",
        "        if path.endswith(\".py\"):\n",
        "            try:\n",
        "                file_asts[path] = ast.parse(content.strip())\n",
        "            except (SyntaxError, ValueError) as e:\n",
        "                print(f\"Warning: Could not parse {path}: {e}\")\n",
        "                continue\n",
        "\n",
        "    for task in task_list:\n",
        "        symbol_name = task.get(\"target_symbol\")\n",
        "        file_path = task.get(\"target_file\")\n",
        "\n",
        "        # --- Generate Metadata ---\n",
        "        class_annotation = \"N/A\"\n",
        "        class_name = \"N/A\"\n",
        "        module_path = file_path.replace('/', '.').replace('.py', '')\n",
        "\n",
        "        # Heuristic: If a symbol starts with a capital letter, it's likely a class.\n",
        "        if symbol_name[0].isupper():\n",
        "            class_annotation = f\"{module_path}.{symbol_name}\"\n",
        "            class_name = class_annotation\n",
        "        # Otherwise, treat it as a function or module-level entity.\n",
        "        else:\n",
        "            class_annotation = f\"{module_path}.{symbol_name}\"\n",
        "\n",
        "        # --- Build the Final Record ---\n",
        "        # The schema now exactly matches the reference benchmark.\n",
        "        final_task_record = {\n",
        "            \"title\": symbol_name,\n",
        "            \"class_annotation\": class_annotation,\n",
        "            \"comment\": task['prompt'].strip(),\n",
        "            \"class_name\": class_name,\n",
        "            \"class_link\": f\"{file_path}\",\n",
        "            \"test_file_path\": task.get(\"test_file_path\"),\n",
        "            \"task_id\": task['task_id']\n",
        "        }\n",
        "        enriched_tasks.append(final_task_record)\n",
        "\n",
        "    return enriched_tasks"
      ],
      "metadata": {
        "id": "u3_7nNlHcQ5X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Generation and Execution\n",
        "\n",
        "This is the main execution cell. It calls the helper functions defined previously to perform the three core generation steps:\n",
        "1.  **Create the physical repository** on the local filesystem.\n",
        "2.  **Generate the `codebase.jsonl` file** from the source code dictionary.\n",
        "3.  **Process and generate the `tasks.jsonl` file** from the enriched task definitions."
      ],
      "metadata": {
        "id": "0m6DQp2yOAKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_physical_repo(repo_root, codebase_content):\n",
        "    \"\"\"Creates the physical directory structure and files on disk.\"\"\"\n",
        "    print(f\"Creating physical repository at: {repo_root}\")\n",
        "    for rel_path, content in codebase_content.items():\n",
        "        full_path = os.path.join(repo_root, rel_path)\n",
        "        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n",
        "        # .strip() to remove potential leading/trailing whitespace from multiline strings\n",
        "        with open(full_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(content.strip())\n",
        "    print(\"Physical repository created successfully.\")\n",
        "\n",
        "def create_codebase_jsonl(output_path, codebase_content):\n",
        "    \"\"\"Creates the codebase.jsonl file with the correct list-of-strings format.\"\"\"\n",
        "    print(f\"Creating codebase JSONL at: {output_path}\")\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        for rel_path, content in codebase_content.items():\n",
        "            entry = {\n",
        "                \"path\": rel_path,\n",
        "                # Use splitlines(True) to match the original benchmark format\n",
        "                \"content\": content.strip().splitlines(keepends=True)\n",
        "            }\n",
        "            # Robust handling for empty files\n",
        "            if not entry[\"content\"] and content.strip() == \"\":\n",
        "                entry[\"content\"] = [\"\"]\n",
        "            elif not entry[\"content\"]:\n",
        "                entry[\"content\"] = [content]\n",
        "            f.write(json.dumps(entry) + '\\n')\n",
        "    print(\"Codebase JSONL created successfully.\")\n",
        "\n",
        "\n",
        "def create_tasks_jsonl(output_path, enriched_task_list):\n",
        "    \"\"\"Creates the tasks.jsonl file from the enriched task data.\"\"\"\n",
        "    print(f\"Creating tasks JSONL at: {output_path}\")\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        for task_record in enriched_task_list:\n",
        "            f.write(json.dumps(task_record) + '\\n')\n",
        "    print(\"Tasks JSONL created successfully.\")\n",
        "\n",
        "# --- Main Execution ---\n",
        "# This block will use the string-based path variables defined in your original Cell 1\n",
        "create_physical_repo(REPO_ROOT_PATH, CODEBASE_CONTENT)\n",
        "print(\"-\" * 20)\n",
        "create_codebase_jsonl(CODEBASE_OUTPUT_PATH, CODEBASE_CONTENT)\n",
        "print(\"-\" * 20)\n",
        "\n",
        "enriched_tasks = enrich_task_definitions(TASK_DEFINITIONS, CODEBASE_CONTENT)\n",
        "create_tasks_jsonl(TASKS_OUTPUT_PATH, enriched_tasks)\n",
        "print(\"\\nBenchmark generation complete with authentic metadata.\")"
      ],
      "metadata": {
        "id": "0jdMJrsxcSpE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "221188f3-17fd-4bc4-f2a5-cba51cfddfa2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating physical repository at: /content/mini_transformers_repo\n",
            "Physical repository created successfully.\n",
            "--------------------\n",
            "Creating codebase JSONL at: /content/mini_transformers_codebase.jsonl\n",
            "Codebase JSONL created successfully.\n",
            "--------------------\n",
            "Creating tasks JSONL at: /content/mini_transformers_tasks.jsonl\n",
            "Tasks JSONL created successfully.\n",
            "\n",
            "Benchmark generation complete with authentic metadata.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Verification and Packaging\n",
        "\n",
        "This final cell performs a sanity check on the generated artifacts to ensure the process was successful. It displays the first few lines of the output `jsonl` files and lists the directory structure of the created repository.\n",
        "\n",
        "Finally, it packages the entire `miniformer_repo` directory into a `.zip` file for easy download and distribution."
      ],
      "metadata": {
        "id": "vBgb0KfmOCJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Verifying Outputs ---\")\n",
        "\n",
        "# Check codebase file\n",
        "print(f\"\\nFirst 2 lines of {CODEBASE_OUTPUT_PATH}:\")\n",
        "!head -n 2 {CODEBASE_OUTPUT_PATH}\n",
        "\n",
        "# Check tasks file\n",
        "print(f\"\\nFirst 2 lines of {TASKS_OUTPUT_PATH}:\")\n",
        "!head -n 2 {TASKS_OUTPUT_PATH}\n",
        "\n",
        "# Check physical repo structure\n",
        "print(f\"\\nPhysical directory structure of {REPO_ROOT_PATH}:\")\n",
        "!ls -R {REPO_ROOT_PATH}\n",
        "\n",
        "# --- Create a downloadable ZIP file of the repository ---\n",
        "print(\"\\n--- Creating ZIP file for GitHub ---\")\n",
        "shutil.make_archive(\"mini_transformers_repo\", 'zip', REPO_ROOT_PATH)\n",
        "print(\"Created mini_transformers_repo.zip. You can download it from the Colab file browser (click the folder icon on the left).\")"
      ],
      "metadata": {
        "id": "KC7Wi0qMdXR9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee1f5ae8-1c95-427a-bddb-862ec9fd8825"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Verifying Outputs ---\n",
            "\n",
            "First 2 lines of /content/mini_transformers_codebase.jsonl:\n",
            "{\"path\": \"main.py\", \"content\": [\"# Main entry point for agent tasks. Initially empty.\"]}\n",
            "{\"path\": \"README.md\", \"content\": [\"# Miniformer\\n\", \"A minimal, educational library for building transformer blocks, designed for agent-based code generation tasks.\\n\", \"This library provides core components for building and experimenting with transformer architectures.\"]}\n",
            "\n",
            "First 2 lines of /content/mini_transformers_tasks.jsonl:\n",
            "{\"title\": \"TransformerConfig\", \"class_annotation\": \"miniformer.config.TransformerConfig\", \"comment\": \"\\\"Add Bias Configuration to TransformerConfig\\\"\\n\\nclass miniformer.config.TransformerConfig\\n\\nModify the configuration class to support toggling bias terms in linear layers.\\n\\n-[ Notes ]-\\nIn some transformer architectures, bias terms in linear projections are considered redundant, especially when followed by a normalization layer like LayerNorm. Providing a configurable flag for this is a common design pattern for architectural experimentation. Your change is required to make the test `test_config_bias_field` in `tests/test_config.py` pass.\\n\\nParameters to Add:\\n  * **use_bias** (*bool*) -- If True, linear layers will include a bias term. This should default to `False` to follow modern best practices.\", \"class_name\": \"miniformer.config.TransformerConfig\", \"class_link\": \"miniformer/config.py\", \"test_file_path\": \"tests/test_config.py\", \"task_id\": \"miniformer-01\"}\n",
            "{\"title\": \"to_numpy\", \"class_annotation\": \"miniformer.utils.tensor_ops.to_numpy\", \"comment\": \"\\\"Verify Tensor Conversion Utility\\\"\\n\\nfunction miniformer.utils.tensor_ops.to_numpy(tensor)\\n\\nVerify that the `to_numpy` utility function is correctly implemented.\\n\\n-[ Description ]-\\nThis function serves as a standard bridge between PyTorch tensors and NumPy arrays, a common operation in ML workflows for debugging, analysis, or interfacing with other libraries. It should handle the device transfer (`.cpu()`) and gradient detachment (`.detach()`) before conversion. The test `test_to_numpy_conversion` in `tests/test_utils.py` validates this behavior. No changes are needed if the implementation is correct.\", \"class_name\": \"N/A\", \"class_link\": \"miniformer/utils/tensor_ops.py\", \"test_file_path\": \"tests/test_utils.py\", \"task_id\": \"miniformer-02\"}\n",
            "\n",
            "Physical directory structure of /content/mini_transformers_repo:\n",
            "/content/mini_transformers_repo:\n",
            "main.py  miniformer  README.md\trequirements.txt  tests\n",
            "\n",
            "/content/mini_transformers_repo/miniformer:\n",
            "activations.py\tconfig.py  __init__.py\tlayers\tmodels\tutils\n",
            "\n",
            "/content/mini_transformers_repo/miniformer/layers:\n",
            "attention.py  feedforward.py  __init__.py\n",
            "\n",
            "/content/mini_transformers_repo/miniformer/models:\n",
            "block.py  __init__.py\n",
            "\n",
            "/content/mini_transformers_repo/miniformer/utils:\n",
            "__init__.py  tensor_ops.py  testing.py\n",
            "\n",
            "/content/mini_transformers_repo/tests:\n",
            "__init__.py\t     test_config.py\t    test_integration.py  test_models.py\n",
            "test_activations.py  test_created_files.py  test_layers.py\t test_utils.py\n",
            "\n",
            "--- Creating ZIP file for GitHub ---\n",
            "Created mini_transformers_repo.zip. You can download it from the Colab file browser (click the folder icon on the left).\n"
          ]
        }
      ]
    }
  ]
}